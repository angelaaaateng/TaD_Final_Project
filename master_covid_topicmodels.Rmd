---
title: "COVID_TopicModel"
author: "Angela Teng"
date: "4/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text as Data Final Project 
Lakshmi Menon, Angela Marie Teng

```{r include = FALSE}
# get current working directory
rm(list = ls())
getwd()  
# setwd("~/Data/articles")

# loading packages 
library(dplyr)
library(ggplot2)
library(xtable)

library(rvest)
library(dplyr)

# Loading multiple packages
libraries <- c("foreign", "stargazer")
lapply(libraries, require, character.only=TRUE)
set.seed(100)
library(quanteda)
library("devtools")
# install.packages("textreadr")
library(textreadr)

# Load it into our environment
library(quanteda.corpora)
# library(quanteda)
# install.packages("readtext")
library(readtext)
# install.packages("spacyr")
library(spacyr)

# to check version
packageVersion("quanteda")

```

Loading in all the articles from rev.com:
```{r data_load, echo =TRUE}
# temp = list.files(pattern="*.txt")
# myfiles = lapply(temp, read.delim)

## the read in of a directory
data = read_dir('/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/articles')

summary(data)
typeof(data)
```

Selecting only articles that contain keywords per country: 
UK - UK, [Boris] Johnson
US - US, Trump 
New Zealand - New Zealand, [Jacinda] Ardern
Australia - Australia, [Scott] Morrison
Canada - Canada, [Justin] Trudeau

(this data is from Data/Articles folder)
```{r subset_data, echo=TRUE}
# select(data$document, matches("trump"))
# data[,grepl("trump", colnames("document"))]
# data[document]

# turn list into df
df <- data.frame(data)
# sanity check
df[1,1:2]

# subset countries by keyword
US_df <- df[grep(c("Donald|Trump|US|United States"), df$document),]
US_df

UK_df <- df[grep(c("Boris|Johnson|United Kingdom|UK"), df$document),]
UK_df


NZ_df <- df[grep(c("Jacinda|Ardern|New Zealand"), df$document),]
NZ_df


AU_df <- df[grep(c("Scott|Morrison|Australia"), df$document),]
AU_df

CA_df <- df[grep(c("Justin|Trudeau|Canada"), df$document),]
CA_df
```





R Webscraping Code 
```{r scrape_text, echo = TRUE}

# 
# scrapetext <- function(url, filename){
#   transcript <- read_html(uk_mar16)
#   
#   text <- transcript %>% 
#     rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>% 
#     xml2::xml_find_all("div/p") %>% 
#     rvest::html_text()
# 
#   speakers <- transcript %>% 
#     rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>% 
#     xml2::xml_find_all("div/p/text()[following-sibling::br]") %>% 
#     rvest::html_text() %>%
#     unique()
#   
#   speaker_fname = paste(filename, 'speakers.txt', sep = '_')
#   text_fname = paste(filename, 'text.txt', sep = '_')
#   write_file(speaker_fname, speakers)
#   write_file(text_fname, text)
# }
# 
# write_file <- function(fname, data){
#   fileConn<-file(fname)
#   writeLines(data, fileConn)
#   close(fileConn)  
# }
# 
# 
# d <- data.frame(urls, fnames)
# for(i in seq_len(nrow(d))) {
#   scrapetext(d[i,1], d[i,2])
# }
```

```{r echo=TRUE}
# *.1 Save workspace after running it -- all objects, functions, etc  (e.g. if you have run something computationally intensive and want to save the object for later use)
# Similar to pickle() in Python

save.image("./workspace_project.RData")

# *.2 Pick up where you left off (but note that the workspace does not include packages. You need packrat for that)

rm(list = ls())

load("./workspace_project.RData")

```




