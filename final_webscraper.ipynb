{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup (BS4) Webscraper for Rev Political Transcripts (COVID-19 Topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bs4 function that gets soup objects\n",
    "\n",
    "def run_bs4(link, lxml=None):\n",
    "    '''\n",
    "    Function that returns soup objects \n",
    "    \n",
    "    Input\n",
    "    ------\n",
    "    link: link that you want to scrape \n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    soup: beautifulsoup object\n",
    "    '''\n",
    "    user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36'\n",
    "    headers = {'User-Agent': user_agent}\n",
    "    page = requests.get(link)\n",
    "    html = page.content\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize empty list of URLs to save rev transcript URLS for all articles\n",
    "list_of_urls = []\n",
    "\n",
    "# loop over all pages of rev website in specific topic dropdown (coronavirus)\n",
    "\n",
    "for i in range(1,44): \n",
    "    soup = run_bs4(str('https://www.rev.com/blog/transcript-tag/coronavirus-update-transcripts/page/' + str(i)))\n",
    "    for a in soup.find_all('a'):\n",
    "        for maybe_transcript_url in (a.get('href').split()):\n",
    "            regex = \"https:\\/\\/www\\.rev\\.com\\/blog\\/transcripts\\/.*\"\n",
    "            matched = re.search(regex, maybe_transcript_url)\n",
    "            if matched:\n",
    "                list_of_urls.append(matched.group(0)) \n",
    "#                 print(len(matched.group(0)))\n",
    "#                 print(matched.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty list of all articles \n",
    "articles = []\n",
    "\n",
    "# for each url, run the bs4 function to get paragraphs\n",
    "for url in list_of_urls: \n",
    "    soup = run_bs4(url)\n",
    "    article = []\n",
    "    for x in soup.find_all('p'):\n",
    "        article.append(x.find_all(text=True))\n",
    "    articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Apr 23, 2020'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Governor Asa Hutchinson of Arkansas held'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[1][1][0][0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for writing in one big text file\n",
    "with open('listfile.txt', 'w') as filehandle:\n",
    "    for listitem in articles:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for article in articles:\n",
    "    with open( article[0][0] + article[1][0][0:40] + '.txt', 'w') as filehandle:\n",
    "        for listitem in article:\n",
    "            filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at only US, Canada, Australia, New Zealand, and the UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    if article[1][1][0][0] contains \n",
    "        with open( article[0][0] + article[1][0][0:40] + '.txt', 'w') as filehandle:\n",
    "            for listitem in article:\n",
    "                filehandle.write('%s\\n' % listitem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
