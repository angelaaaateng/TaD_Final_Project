df[nrow(df) + 1,] = c(northabbey, 'Northanger Abbey',"Jane Austen")
View(df)
x = c(northabbey, 'Northanger Abbey',"Jane Austen")
rbind(df, x)
rbind(df, data.frame('text' = northabbey, 'title' = 'Northanger Abbey', 'author' = 'Jane Austen'))
View(df)
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Northanger Abbey', 'author' = 'Jane Austen'))
View(df)
persuasion <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[1])%>%get_excerpt() %>% paste(collapse = ' ') %>L% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
persuasion <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[1])%>%get_excerpt() %>% paste(collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
rm(list = ls())
# Libraries
library(dplyr)
library(quanteda)
library(quanteda.corpora)
library(gutenbergr)
library(tidytext)
austen_ids = slice(gutenberg_works(author == 'Austen, Jane'), 0:4)
dickens_ids = slice(gutenberg_works(author == 'Dickens, Charles'), 0:4)
alcott_ids = slice(gutenberg_works(author == 'Alcott, Louisa May'), 0:4)
bronte_ids = slice(gutenberg_works(author == 'Brontë, Charlotte'), 0:4)
get_excerpt <- function(book) {
book <- lapply(book, function(x) x[!x %in% ""])
book <- book[[2]][1:500]
return(book)
}
create_df <- function(entry, text, title, author) {
book <- paste(book, collapse = ' ') %>% data.frame('text' = text, 'title' = title, 'author' = author)
return(book)
}
persuasion <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[1])%>%get_excerpt()
northabbey <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[2])%>%get_excerpt()
mans_park <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()
emma <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()
chris_carol <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[1])%>%get_excerpt() %>% corpus()
two_cities <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[2])%>%get_excerpt() %>% corpus()
edwin  <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()
pickwick <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()
fables <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[1])%>%get_excerpt() %>% corpus()
women <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[2])%>%get_excerpt() %>% corpus()
cousins <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()
jacknjill <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()
prof <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[1])%>%get_excerpt() %>% corpus()
eyre <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[2])%>%get_excerpt() %>% corpus()
villette <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()
shirley <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()
i = 1
book_list <- c(c(persuasion, northabbey, mans_park, emma),
c(chris_carol, two_cities, edwin, pickwick),
c(fables, women, cousins, jacknjill),
c(prof, eyre, villette, shirley))
create_df <- function(text, title, author) {
text <- paste(text, collapse = ' ') %>% data.frame('text' = text, 'title' = title, 'author' = author)
return(book)
}
persuasion <- create_df(persuasion, 'Persuasion', 'Jane Austen' )
create_df <- function(text, title, author) {
text <- paste(text, collapse = ' ') %>% data.frame('text' = text, 'title' = title, 'author' = author)
return(text)
}
persuasion <- create_df(persuasion, 'Persuasion', 'Jane Austen' )
View(persuasion)
cdc <- paste(fables,collapse=" ")
typeof(cdc)
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Northanger Abbey', 'author' = 'Jane Austen'))
cdc <- paste(fables,collapse=" ")
persuasion <- paste(text, collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
persuasion_df <- paste(text, collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
persuasion_df <- paste(persuasion, collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Northanger Abbey', 'author' = 'Jane Austen'))
persuasion_df <- paste(persuasion, collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
chris_carol <- paste(persuasion, collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
two_cities <- paste(persuasion, collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
persuasion_df <- paste(persuasion, collapse = ' ') %>% data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
northabbey_df <- paste(northabbey, collapse = ' ') %>% data.frame('text' = northabbey, 'title' = 'Northanger Abbey', 'author' = 'Jane Austen')
df <- rbind(persuasion_df, northabbey_df)
cdc <- paste(fables,collapse=" ")
persuasion <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[1])%>%get_excerpt() %>% paste(collapse = ' ')
rm(list = ls())
# Libraries
library(dplyr)
library(quanteda)
library(quanteda.corpora)
library(gutenbergr)
library(tidytext)
austen_ids = slice(gutenberg_works(author == 'Austen, Jane'), 0:4)
dickens_ids = slice(gutenberg_works(author == 'Dickens, Charles'), 0:4)
alcott_ids = slice(gutenberg_works(author == 'Alcott, Louisa May'), 0:4)
bronte_ids = slice(gutenberg_works(author == 'Brontë, Charlotte'), 0:4)
get_excerpt <- function(book) {
book <- lapply(book, function(x) x[!x %in% ""])
book <- book[[2]][1:500]
return(book)
}
persuasion <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[1])%>%get_excerpt() %>% paste(collapse = ' ')
northabbey <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[2])%>%get_excerpt() %>% paste(collapse = ' ')
mans_park <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
emma <- gutenberg_download(gutenberg_id = austen_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
chris_carol <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[1])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
two_cities <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[2])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
edwin  <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
pickwick <- gutenberg_download(gutenberg_id = dickens_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
fables <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[1])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
women <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[2])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
cousins <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
jacknjill <- gutenberg_download(gutenberg_id = alcott_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
prof <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[1])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
eyre <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[2])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
villette <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[3])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
shirley <- gutenberg_download(gutenberg_id = bronte_ids$gutenberg_id[4])%>%get_excerpt() %>% corpus()%>% paste(collapse = ' ')
df <- data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Northanger Abbey', 'author' = 'Jane Austen'))
p = corpus(persuasion)
df <- data.frame('text' = persuasion, 'title' = 'Persuasion', 'author' = 'Jane Austen')
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Northanger Abbey', 'author' = 'Jane Austen'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Mansfield Park', 'author' = 'Jane Austen'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Emma', 'author' = 'Jane Austen'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'A Christmas Carol in Prose; Being a Ghost Story of Christmas', 'author' = 'Charles Dickens'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'A Tale of Two Cities', 'author' = 'Charles Dickens'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'The Mystery of Edwin Drood', 'author' = 'Charles Dickens'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'The Pickwick Papers', 'author' = 'Charles Dickens'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Flower Fables', 'author' = 'Louisa May Alcott'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Little Women', 'author' = 'Louisa May Alcott'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Eight Cousins', 'author' = 'Louisa May Alcott'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Jack and Jill', 'author' = 'Louisa May Alcott'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'The Professor', 'author' = 'Charlotte Bronte'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Jane Eyre: An Autobiography', 'author' = 'Charlotte Bronte'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Villette', 'author' = 'Charlotte Bronte'))
df<- rbind(df, data.frame('text' = northabbey, 'title' = 'Shirley', 'author' = 'Charlotte Bronte'))
View(df)
#----------------------------------------
# 3 Applying Naive Bayes and Word Scores to Amicus texts from Evans et al ---
#----------------------------------------
# Loading data
data("data_corpus_amicus")
# load required libraries
library(quanteda)
library(quanteda.corpora)
#----------------------------------------
# 3 Applying Naive Bayes and Word Scores to Amicus texts from Evans et al ---
#----------------------------------------
# Loading data
data("data_corpus_amicus")
#----------------------------------------
# 3 Applying Naive Bayes and Word Scores to Amicus texts from Evans et al ---
#----------------------------------------
# Loading data
View(data("data_corpus_amicus"))
knitr::opts_chunk$set(echo = TRUE)
reviews <- read.csv(file = 'yelp.csv')
reviews <- read.csv(file = 'yelp.csv')
median_stars <- median(reviews$stars)
median_stars <- median(reviews$stars)
reviews$label <- ifelse(reviews$stars>=4, 'positive', 'negative')
reviews$label <- ifelse(reviews$stars>=4, 'positive', 'negative')
reviews_short <- reviews[1:1000,]
reviews_short$text <- gsub(pattern = "'", "", reviews_short$text)  # replace apostrophes
prop.table(table(reviews_short$label))
reviews_short <- reviews_short %>% sample_n(nrow(reviews_short))
library(dplyr)
reviews_short <- reviews_short %>% sample_n(nrow(reviews_short))
rownames(reviews_short) <- NULL
reviews_dfm <- dfm(reviews_short$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english")) %>% convert("matrix")
library(quanteda)
library(quanteda)
library(quanteda.corpora)
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
reviews_dfm <- dfm(reviews_short$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english")) %>% convert("matrix")
run_svm <- function(prop, method = 'svmLinear'){
ids_train <- createDataPartition(1:nrow(reviews_dfm), p = prop, list = FALSE, times = 1)
train_x <- reviews_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- reviews_short$label[ids_train] %>% as.factor()  # train set labels
val_x <- reviews_dfm[-ids_train, ]  %>% as.data.frame() # test set data
val_y <- reviews_short$label[-ids_train] %>% as.factor() # test set labels
trctrl <- trainControl(method = "cv",
number = 5)
svm_mod <- train(x = train_x,
y = train_y,
method = method,
trControl = trctrl)
svm_pred <- predict(svm_mod, newdata = val_x)
svm_cmat <- confusionMatrix(svm_pred, val_y, positive = 'positive')
#print(svm_cmat)
return(svm_cmat$overall[["Accuracy"]])
}
acc_20 <- run_svm(0.2)
acc_20 <- run_svm(0.2)
acc_50 <- run_svm(0.5)
acc_70 <- run_svm(0.7)
cat('\n',
"SVM-Linear Accuracy 20%:",  acc_20, '\n',
"SVM-Linear Accuracy 50%:",  acc_50, '\n',
"SVM-Linear Accuracy 70%:",  acc_70, '\n'
)
acc_radial <- run_svm(0.7, 'svmRadial')
acc_radial <- run_svm(0.7, 'svmRadial')
cat("\n SVM-Linear Accuracy 50%:",  acc_radial, '\n')
library(text2vec)
knitr::opts_chunk$set(echo = TRUE)
# import libraries
rm(list = ls())
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Homework/Homework3")
libraries <- c("dplyr", "quanteda", "quanteda.corpora", "ldatuning", "topicmodels",
"ggplot2", "stm", "tm", "factoextra", "text2vec", "lsa", "bursts")
lapply(libraries, require, character.only = T)
# import libraries
rm(list = ls())
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Homework/Homework3")
libraries <- c("dplyr", "quanteda", "quanteda.corpora", "ldatuning", "topicmodels",
"ggplot2", "stm", "tm", "factoextra", "text2vec", "lsa", "bursts")
lapply(libraries, require, character.only = T)
news <- corpus_subset(data_corpus_immigrationnews,
paperName %in% c("telegraph", "guardian", "ft", "independent", "express"))
table(news$documents$paperName)
load('custom_stopwords.RData')
news_dfm <- dfm(news, remove = custom_stopwords, remove_punct = TRUE, stem = TRUE, tolower = TRUE, remove_numbers = TRUE)
news_dfm <- dfm_trim(news_dfm, min_termfreq = 30, min_docfreq = 20)
news_dfm
##### MODEL CODE #####
#news_tm <- LDA(news_dfm, k = 25, method = "Gibbs",  control = list(seed = 1234), iter = 3000)
#saveRDS(news_tm, 'news_tm')
#save.image('myworkspace.RData')
load('myworkspace.RData')
news_tm@loglikelihood
top_terms <- get_terms(news_tm, 10)
print(top_terms)
likely_topic <- topics(news_tm)
topics <- data.frame(table(likely_topic))
topics <- topics[order(topics$Freq, decreasing = TRUE),]
row.names(topics) <- NULL
print(topics)
top_5 <- c(topics$likely_topic[1:5])
for(topic in top_5){
top <- paste('Topic', topic)
print(top)
print(top_terms[,top])
}
which.max2 <- function(x){
which(x == sort(x,partial=(25-1))[25-1])
}
plot_top2 <-function(name){
ft <- which(news$documents$paperName == name)
first <- ft[1]
last <- tail(ft, n=1)
doc_topics <- news_tm@gamma[first:last,]
days <- as.numeric(news$documents$day[first:last])
doc_topics <- t(doc_topics)
max <- apply(doc_topics, 2, which.max)
max2 <- apply(doc_topics, 2, which.max2)
max2 <- sapply(max2, max)
top2 <- data.frame(top_topic = max, second_topic = max2, date = days)
top2 <- top2[order(top2$date),]
title = paste("Top 2 Topics for", name)
ft_plot <- ggplot(top2, aes(x=date, y=top_topic, pch="First"))
ft_plot + geom_point(aes(x=date, y=second_topic, pch="Second") ) +theme_bw() +
ylab("Topic Number") + ggtitle(title) + geom_point() + xlab(NULL) +
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
}
plot_top2('ft')
plot_top2('guardian')
names = c("telegraph", "guardian", "ft", "independent", "express")
data = c()
i=1
for(name in names){
vals <- which(news$documents$paperName == name)
first <- vals[1]
last <- tail(vals, n=1)
mean_vals = c()
for(topic in top_5){
doc_topics <- news_tm@gamma[first:last, topic]
mean_vals <- append(mean_vals, mean(doc_topics))
}
data[[i]] = mean_vals
i = i+1
}
tab <- as.data.frame(data, row.names = names, col.names = top_5)
print(tab)
#news_tm_2 <- LDA(news_dfm, k = 25, method = "Gibbs",  control = list(seed = 2000), iter = 3000)
#saveRDS(news_tm_2, 'news_tm_q2a')
#save.image('myworkspace_q2.RData')
load('myworkspace_q2.RData')
news_tm_2@loglikelihood
q2 <- function(tm1, tm2){
dist_1 <- tm1@beta
dist_2 <- tm2@beta
data2 = c()
for(row2 in 1:nrow(dist_2)){
sims <- c()
for(row1 in 1:nrow(dist_1)){
cosine_val <- cosine(dist_2[row2, ], dist_1[row1,])
sims <- append(sims, cosine_val)
}
data2[[row2]] = (which.max(sims))
}
cosine_sim <- data.frame(data2)
colnames(cosine_sim) <- 'Model1_Topic'
cosine_sim$'Model2_Topic' <- rownames(cosine_sim)
cosine_sim
top_terms1 <- get_terms(tm1, 10)
top_terms2 <- get_terms(tm2, 10)
commons<- c()
for(topic in 1:nrow(cosine_sim)){
list2 <- top_terms2[, topic]
list1 <- top_terms1[, cosine_sim[topic,1] ]
commons <- append(commons, (length(intersect(list1, list2))))
}
print(cosine_sim)
cosine_sim$common <- commons
return(cosine_sim)
}
sim_table <- q2(news_tm, news_tm_2)
print(sim_table)
cat('Average Common Words', mean(sim_table$common))
##### MODEL CODE #####
#news_tm_5_1234 <- LDA(news_dfm, k = 5, method = "Gibbs",  control = list(seed = 1234), iter = 3000)
#news_tm_5_2000 <- LDA(news_dfm, k = 5, method = "Gibbs",  control = list(seed = 2000), iter = 3000)
#saveRDS(news_tm_5_1234, 'news_tm_5_1234')
#saveRDS(news_tm_5_2000, 'news_tm_5_2000')
#save.image('myworkspace_q2d.RData')
load('myworkspace_q2d.RData')
news_tm_5_1234@loglikelihood
news_tm_5_2000@loglikelihood
sim_table2 <- q2(news_tm_5_1234, news_tm_5_2000)
cat('Average Common Words', mean(sim_table2$common))
##### MODEL CODE #####
#news_stm_content <- stm(news_dfm, K=0, prevalence = ~paperName + s(date), content = ~paperName, init.type = 'Spectral', seed = 2000)
#saveRDS(news_stm_content, 'news_stm_content')
#save.image('myworkspace_q3.RData')
load('myworkspace_q3.RData')
plot(news_stm_content, type = 'summary')
plot(news_stm_content, type="perspectives", topics = 49)
prep <- estimateEffect(c(49) ~paperName + s(date) , news_stm_content, metadata = news$documents)
plot(prep, "date", news_stm_content, topics = 49,
method = "continuous", xaxt = "n", xlab = "Date")
data <- corpus_subset(data_corpus_ukmanifestos, Party %in% c('Con', 'Lab'))
lab_con_dfm <- dfm(data,
stem = T,
remove = stopwords("english"),
remove_punct = T)
lab_index <- which(data$documents$Year == 1979 & data$documents$Party == 'Lab' )
con_index <- which(data$documents$Year == 1979 & data$documents$Party == 'Con' )
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(lab_index, con_index)) # second parameter corresponds to index texts
textplot_scale1d(manifestos_fish, groups = data$party)
summary(manifestos_fish)
words <- manifestos_fish$psi
weights <- manifestos_fish$beta
plot(weights, words)
x <- manifestos_fish$theta
y <- ifelse(data$documents$Party == 'Lab', 1, 0)
lin_model <- lm(y ~ x, data = lab_con_dfm)
summary(lin_model)
## From Lab 13
bursty <- function(word, DTM, date) {
word.vec <- DTM[, which(colnames(DTM) == word)]
if(length(word.vec) == 0) {
print(paste(word, " does not exist in this corpus."))
return()
}
else {
word.times <- c(0,which(as.vector(word.vec)>0))
kl <- kleinberg(word.times, gamma = 0.5)
kl$start <- date[kl$start+1]
kl$end <- date[kl$end]
max_level <- max(kl$level)
plot(c(kl$start[1], kl$end[1]), c(1,max_level),
type = "n", xlab = "Time", ylab = "Level", bty = "n",
xlim = c(min(date), max(date)), ylim = c(1, max_level),
yaxt = "n")
axis(2, at = 1:max_level)
for (i in 1:nrow(kl)) {
if (kl$start[i] != kl$end[i]) {
arrows(kl$start[i], kl$level[i], kl$end[i], kl$level[i], code = 3, angle = 90,
length = 0.05)
}
else {
points(kl$start[i], kl$level[i])
}
}
print(kl)
}
}
news <- readRDS('news_data.rds')
news_corpus <- corpus(news$headline)
news_dfm <- dfm(news_corpus)
bursty("obama", news_dfm, as.Date(news$date))
bursty("korea", news_dfm, as.Date(news$date))
bursty("afghanistan", news_dfm, as.Date(news$date))
news_data <- readRDS("news_data.rds")
table(news_data$category)
set.seed(2000)
news_samp <- news_data %>%
filter(category %in% c("WORLD NEWS")) %>%
group_by(category) %>%
sample_n(1000) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
news_dfm <- dfm(news_samp$text, tolower = TRUE, remove_punct = TRUE, remove = stopwords("english"))
news_mat <- convert(news_dfm, to = "matrix")
news_pca <- prcomp(news_mat, center = TRUE, scale = TRUE)
pc_loadings <- news_pca$rotation
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, 5, loading),top_n(pc1_loading, -5, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
pc1_loading
news_mat_lsa <- convert(news_dfm, to = "lsa") # convert to transposed matrix (so terms are rows and columns are documents = TDM)
news_mat_lsa <- lw_logtf(news_mat_lsa) * gw_idf(news_mat_lsa) # local - global weighting (akin to TFIDF)
news_lsa <- lsa(news_mat_lsa)
news_lsa_mat <- as.textmatrix(news_lsa)
america <- associate(news_lsa_mat, "america", "cosine", threshold = .7)
america[1:5]
corruption <- associate(news_lsa_mat, "corruption", "cosine", threshold = .6)
corruption[1:5]
pretrained <- readRDS("glove.rds") # GloVe pretrained (https://nlp.stanford.edu/projects/glove/)
# function to compute nearest neighbors
nearest_neighbors <- function(cue, embeds, N = 5, norm = "l2"){
cos_sim <- sim2(x = embeds, y = embeds[cue, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # cue is always the nearest neighbor hence dropped
}
america_glove <- nearest_neighbors("state", pretrained, N = 5, norm = "l2")
america_glove
corruption_glove <- nearest_neighbors("welfare", pretrained, N = 5, norm = "l2")
corruption_glove
rm(list = ls())
library(rvest)
library(dplyr)
nz_apr6 = 'https://www.rev.com/blog/transcripts/new-zealand-covid-19-briefing-transcript-april-6'
scrapetext <- function(url, filename){
transcript <- read_html(uk_mar16)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[following-sibling::br]") %>%
rvest::html_text()
#speakers <- transcript %>%
# rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
#xml2::xml_find_all("div/p/text()[following-sibling::br]") %>%
#rvest::html_text() %>%
#unique()
#speaker_fname = paste(filename, 'speakers.txt', sep = '_')
text_fname = paste(filename, 'text2.txt', sep = '_')
#write_file(speaker_fname, speakers)
write_file(text_fname, text)
}
scrapetext <- function(url, filename){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[following-sibling::br]") %>%
rvest::html_text()
#speakers <- transcript %>%
# rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
#xml2::xml_find_all("div/p/text()[following-sibling::br]") %>%
#rvest::html_text() %>%
#unique()
#speaker_fname = paste(filename, 'speakers.txt', sep = '_')
text_fname = paste(filename, 'text2.txt', sep = '_')
#write_file(speaker_fname, speakers)
write_file(text_fname, text)
}
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
close(fileConn)
}
scrapetext(nz_apr6, 'nz_apr_6')
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
print('Saved')
close(fileConn)
}
scrapetext(nz_apr6, 'nz_apr_6')
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
print(fileConn)
print('Saved')
close(fileConn)
}
scrapetext(nz_apr6, 'nz_apr_6')
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Project/TaD_Final_Project/Data")
scrapetext(nz_apr6, 'nz_apr_6')
scrapetext <- function(url, filename){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
#speakers <- transcript %>%
# rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
#xml2::xml_find_all("div/p/text()[following-sibling::br]") %>%
#rvest::html_text() %>%
#unique()
#speaker_fname = paste(filename, 'speakers.txt', sep = '_')
text_fname = paste(filename, 'text2.txt', sep = '_')
#write_file(speaker_fname, speakers)
write_file(text_fname, text)
}
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
print('Saved')
close(fileConn)
}
scrapetext(nz_apr6, 'nz_apr_6')
