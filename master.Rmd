---
title: "COVID_TopicModel"
author: "Angela Teng"
date: "4/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Text as Data Final Project 
Lakshmi Menon, Angela Marie Teng
```{r echo=TRUE}

# get current working directory
rm(list = ls())
getwd()  
# setwd("~/Data/articles")

```

```{r include = FALSE}


# loading packages 
library(dplyr)
library(ggplot2)
library(xtable)
library(plyr)

library(rvest)
library(dplyr)
# install.packages("proxy")
library(proxy)
# Loading multiple packages
libraries <- c("foreign", "stargazer")
lapply(libraries, require, character.only=TRUE)
set.seed(100)
library(quanteda)
library("devtools")
# install.packages("textreadr")
library(textreadr)

library(rjson)

# Load it into our environment
library(quanteda.corpora)
library(quanteda)
# install.packages("tm")
library(tm)
library(readtext)
# install.packages("spacyr")
library(spacyr)

libraries <- c("topicmodels", "dplyr", "stm", "quanteda")
lapply(libraries, require, character.only = T)
# to check version
packageVersion("quanteda")
# install.packages("magicfor")
library(magicfor)
```

Amber Notes: 
- So I think we can create and use different DFMs with different pre-processing methods and compare that instead of using just 1 dfm. It would be cool to see how preprocessing affects our results 

**DFMS:**
dfm <- dfm of entire corpus; dfm(covid_corp, tolower=TRUE, stem = TRUE,  remove = stopwords("english"), remove_punct = TRUE)
dfm_trimmed <- dfm_trim(dfm, min_termfreq=20, min_docfreq=2)

dfm_2 <- removing numbers from here 

## Preprocessing 

Loading in all the articles from rev.com:
```{r data_load, echo =TRUE}
# temp = list.files(pattern="*.txt")
# myfiles = lapply(temp, read.delim)

## the read in of a directory
data = read_dir('/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/articles')

summary(data)
typeof(data)
attributes(data)
```

Selecting only articles that contain keywords per country: 
UK - UK, [Boris] Johnson
US - US, Trump 
New Zealand - New Zealand, [Jacinda] Ardern
Australia - Australia, [Scott] Morrison
Canada - Canada, [Justin] Trudeau

(this data is from Data/Articles folder)

Load in scraped data: 

```{r data_scraped, echo = TRUE}
load('./Workspaces/scraped_dates.RData')
summary(matches)

matches$country <- ifelse(grepl("Donald|Trump|US|United States", matches$title), "US", "country")

matches$country <- ifelse(grepl("Donald|Trump|US|United States", matches$title), "US", matches$country)

matches$country <- ifelse(grepl("Boris|Johnson|United Kingdom|UK", matches$title), "UK", matches$country)

matches$country <- ifelse(grepl("Jacinda|Ardern|New Zealand", matches$title), "NZ", matches$country)

matches$country <- ifelse(grepl("Scott|Morrison|Australia", matches$title), "AU", matches$country)

matches$country <- ifelse(grepl("Justin|Trudeau|Canada", matches$title), "CA", matches$country)

head(matches)

colnames(matches)
typeof(matches)
attributes(matches)

```

```{r subset_data, echo=TRUE}
matches[,grepl("trump", colnames("title"))]

# turn list into df
df <- data.frame(matches)
# sanity check
df[1,1:2]

```

Now, we want to preprocess our text and convert it into a dfm
```{r preprocessing, echo = TRUE}

# turn list into df
df <- as.data.frame(matches, stringsAsFactors=F)

attributes(df)

typeof(df)
typeof(df$text)

mode(df$text)
storage.mode(df$text) <- "character"

covid_corp <- corpus(df, text_field ="text" )

df <- apply(df,2,as.character)
head(df)

write.csv(df,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/master.csv")


head(covid_corp)
# a corpus consists ofs: (1) documents: text + doc level data (2) corpus metadata (3) extras (settings)
head(docvars(covid_corp))  # document-level variables
metacorpus(covid_corp)  # corpus-level variables

# ndoc identifies the number of documents in a corpus
ndocs <- ndoc(covid_corp)
# this is because it's subdividedd into paragraph, rather than article.....
ndocs

corpusinfo <- summary(covid_corp, n = ndocs)  # note n default is 100
head(corpusinfo)
```



STM recommended preprocessing from paper (this takes super long to run)
```{r echo = TRUE}
# STM Recommended preprocessing from paper

# data <- read.csv("./Data/master.csv")
# head(data)
length(covid_corp$documents)

# note: this code is very slow
# apply STM Vignette recommended preprocessing
# processed <- textProcessor(covid_corp$documents)

# save.image("./Workspaces/STM_preprocessing.RData")
# load("./Workspaces/STM_preprocessing.RData")
# out<- prepDocuments(processed$documents, processed$vocab, processed$meta)
# docs <- out$documents
# vocab <- out$vocab

# @Amber question: Should we remove numbers in our preprocessing?

```

## DFM Creation 

Quanteda Preprocessing and DFM creation
```{r echo = TRUE}

# Using data from the corpus
tokens <- tokens(covid_corp, remove_punct = TRUE, remove_numbers = FALSE, remove_symbols = TRUE, remove_url = TRUE,  remove_hyphens=TRUE) 

# tokens
num_tokens <- sum(lengths(tokens))
num_tokens

dfm <- dfm(covid_corp, tolower=TRUE, stem = TRUE,  remove = stopwords("english"), remove_punct = TRUE)
dfm

# remove speaker from stopwords

tad_stopwords <- c("speaker","inaudible", "foreign language")

?textProcessor
dfm_2 <- dfm(covid_corp, tolower=TRUE, stem = TRUE,  remove = c(stopwords("english"), tad_stopwords), remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE, remove_hyphens = TRUE, remove_symbols = TRUE)
dfm_2

save.image("./Workspaces/dfm_2.RData")

```

Basic EDA by country - the countries that say country aren't associated with our keywords (do not have our keywords in the title)
```{r dfm, echo=TRUE}
head(docvars(covid_corp))

# Create a table that shows how many documents are associated with each newspaper
# this works for R 3.5 but not 3.6 
associated_docs <- aggregate(texts ~ country, data=as.data.frame(covid_corp$documents), FUN=length)
associated_docs

table(covid_corp$documents$country)
# country is probably NA (meaning the title didn't match any of the key words)

# remove  words  that  occur  fewer  than  20  times  or in  fewer  than  2  documents
dfm_trimmed <- dfm_trim(dfm, min_termfreq=20, min_docfreq=2)
head(dfm_trimmed)
# total num of features after processing
nfeat(dfm_trimmed)
# total num of documents 
ndoc(dfm_trimmed)


dfm_trimmed_2 <- dfm_trim(dfm_2, min_termfreq = 20, min_docfreq = 2)
head(dfm_trimmed_2)
nfeat(dfm_trimmed_2)
ndoc(dfm_trimmed_2)
save.image("./Workspaces/dfm_2_new.RData")
```

## LDA 

### LDA Topic Modeling
```{r lda, echo=TRUE}
lda_tm <- LDA(dfm_trimmed, k = 25, method = "Gibbs", iter=3000, control = list(seed = 1234))
lda_tm
save.image("./Workspaces/LDA_topicmodel.RData")

log_likelihood_lda <- lda_tm@loglikelihood
log_likelihood_lda

# get top 10 words that contribute to each topic 
top10_words_lda <- get_terms(lda_tm, k=10)
top10_words_lda

# save the  top  10  words  over  all  25  topics,  for  later  use.
save(top10_words_lda, file = "./Data/top10words_lda.Rdata")

save.image("./Workspaces/LDA_topicmodel_top10words.RData")

# find most likely topic for each country
most_likely_topics_lda <- topics(lda_tm)
most_likely_topics_lda

lda_topics <- lda_tm@gamma
lda_topics <- t(lda_topics)

most_prob_topic_lda <- apply(lda_topics, 2, which.max)
table_prob_topics_lda <- table(most_prob_topic_lda)
table_prob_topics_lda

# sort the table
top_transcript_topics_lda <- sort(table(most_prob_topic_lda), decreasing=TRUE)
top_transcript_topics_lda
top_terms_lda_20 <- get_terms(lda_tm, k=20) 
top_terms_lda_20 

write.csv(top_terms_lda_20,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_20.csv")

save.image("./Workspaces/LDA_topicmodel_master.RData")

top_terms_lda_50 <- get_terms(lda_tm, k=50) 
top_terms_lda_50 
write.csv(top_terms_lda_50,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_50.csv")


```

### DFM 2 - LDA
```{r lda2, echo = TRUE}
# lda on DFM 2
lda_tm_2 <- LDA(dfm_trimmed_2, k = 25, method = "Gibbs", iter=3000, control = list(seed = 1234))
lda_tm_2

save.image("./Workspaces/LDA_topicmodel_2.RData")

log_likelihood_lda_2 <- lda_tm_2@loglikelihood
log_likelihood_lda_2

# get top 10 words that contribute to each topic 
top10_words_lda_2_2 <- get_terms(lda_tm_2, k=10)
top10_words_lda_2_2

# save the  top  10  words  over  all  25  topics,  for  later  use.
save(top10_words_lda_2_2, file = "./Data/top10words_lda_2.Rdata")

save.image("./Workspaces/LDA_topicmodel_top10words_2.RData")

# find most likely topic for each country
most_likely_topics_lda_2 <- topics(lda_tm_2)
most_likely_topics_lda_2

lda_topics_2 <- lda_tm_2@gamma
lda_topics_2 <- t(lda_topics_2)

most_prob_topic_lda_2 <- apply(lda_topics_2, 2, which.max)
table_prob_topics_lda_2 <- table(most_prob_topic_lda_2)
table_prob_topics_lda_2

# sort the table
top_transcript_topics_lda_2 <- sort(table(most_prob_topic_lda_2), decreasing=TRUE)
top_transcript_topics_lda_2
top_terms_lda_20_2 <- get_terms(lda_tm_2, k=20) 
top_terms_lda_20_2 

write.csv(top_terms_lda_20_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_20_2.csv")

save.image("./Workspaces/LDA_topicmodel_master_2.RData")

top_terms_lda_50_2 <- get_terms(lda_tm_2, k=50) 
top_terms_lda_50_2
write.csv(top_terms_lda_50_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/top_terms_lda_50_2.csv")


```

### Examine top contribuuting topics
```{r top_lda, echo = True}

top_topics_lda <- data.frame(title = covid_corp$documents$title, country = covid_corp$documents$country, top_topic = most_prob_topic_lda, date = covid_corp$documents$date)
top_topics_lda

save(top_topics_lda, file = "./Data/top_topics_lda.Rdata")
save.image("./Workspaces/top_lda_topics.RData")

write.csv(top_topics_lda,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/lda_topics.csv")

# for dfm 2
top_topics_lda_2 <- data.frame(title = covid_corp$documents$title, country = covid_corp$documents$country, top_topic = most_prob_topic_lda_2, date = covid_corp$documents$date)
top_topics_lda_2

save(top_topics_lda_2, file = "./Data/top_topics_lda_2.Rdata")
save.image("./Workspaces/top_lda_topics_2.RData")

write.csv(top_topics_lda_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/lda_topics_2.csv")

```




```{r more_lda, echo = TRUE}
load("./Workspaces/top_lda_topics.RData")

# find this for each country
US <- top_topics_lda[top_topics_lda$country == "US" ,]
UK <- top_topics_lda[top_topics_lda$country == "UK" ,]
AU <- top_topics_lda[top_topics_lda$country == "AU" ,]
NZ <- top_topics_lda[top_topics_lda$country == "NZ" ,]
CA <- top_topics_lda[top_topics_lda$country == "CA" ,]
unclassified <- top_topics_lda[top_topics_lda$country == "country" ,]

# order by date column
US$date <- as.numeric(US$date)
UK$date <- as.numeric(UK$date)
AU$date <- as.numeric(AU$date)
NZ$date <- as.numeric(NZ$date)
CA$date <- as.numeric(CA$date)

# plot the terms for only the top 1 topic
# US_plot
US_plot <- ggplot(US, aes(x=date, y=top_topic, pch="Top Topic")) 
US_plot +theme_bw() + ylab("Topic Number") + ggtitle("US Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# UK_plot
UK_plot <- ggplot(UK, aes(x=date, y=top_topic, pch="Top Topic")) 
UK_plot +theme_bw() + ylab("Topic Number") + ggtitle("UK Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# AU_plot
AU_plot <- ggplot(AU, aes(x=date, y=top_topic, pch="Top Topic")) 
AU_plot +theme_bw() + ylab("Topic Number") + ggtitle("AU Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# NZ_plot
NZ_plot <- ggplot(NZ, aes(x=date, y=top_topic, pch="Top Topic")) 
NZ_plot +theme_bw() + ylab("Topic Number") + ggtitle("NZ Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# US_plot
CA_plot <- ggplot(CA, aes(x=date, y=top_topic, pch="Top Topic")) 
CA_plot +theme_bw() + ylab("Topic Number") + ggtitle("CA Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

save.image("./Workspaces/top_lda_topic_plots.RData")

```

### Lda for DFM 2
```{r echo = TRUE}

load("./Workspaces/top_lda_topics_2.RData")

# find this for each country
US <- top_topics_lda_2[top_topics_lda_2$country == "US" ,]
UK <- top_topics_lda_2[top_topics_lda_2$country == "UK" ,]
AU <- top_topics_lda_2[top_topics_lda_2$country == "AU" ,]
NZ <- top_topics_lda_2[top_topics_lda_2$country == "NZ" ,]
CA <- top_topics_lda_2[top_topics_lda_2$country == "CA" ,]
unclassified <- top_topics_lda_2[top_topics_lda_2$country == "country" ,]

# order by date column
US$date <- as.numeric(US$date)
UK$date <- as.numeric(UK$date)
AU$date <- as.numeric(AU$date)
NZ$date <- as.numeric(NZ$date)
CA$date <- as.numeric(CA$date)

# plot the terms for only the top 1 topic
# US_plot
US_plot_2 <- ggplot(US, aes(x=date, y=top_topic, pch="Top Topic")) 
US_plot +theme_bw() + ylab("Topic Number") + ggtitle("US Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# UK_plot
UK_plot <- ggplot(UK, aes(x=date, y=top_topic, pch="Top Topic")) 
UK_plot +theme_bw() + ylab("Topic Number") + ggtitle("UK Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# AU_plot
AU_plot <- ggplot(AU, aes(x=date, y=top_topic, pch="Top Topic")) 
AU_plot +theme_bw() + ylab("Topic Number") + ggtitle("AU Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# NZ_plot
NZ_plot <- ggplot(NZ, aes(x=date, y=top_topic, pch="Top Topic")) 
NZ_plot +theme_bw() + ylab("Topic Number") + ggtitle("NZ Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

# US_plot
CA_plot <- ggplot(CA, aes(x=date, y=top_topic, pch="Top Topic")) 
CA_plot +theme_bw() + ylab("Topic Number") + ggtitle("CA Top Topics") + geom_point() + xlab(NULL) + scale_shape_manual(values=c(19, 1), name = "Topic Rank")

save.image("./Workspaces/top_lda_topic_plots_2.RData")




```


### Find the contribution of each LDA topic to each country's top topics
```{r topic_contrib, echo = TRUE}
topics_df_lda <- data.frame(t(data.frame(lda_topics)))
head(topics_df_lda)

save(topics_df_lda, file = "./Data/top_topics_lda_df.Rdata")
save.image("./Workspaces/top_lda_topics_df.RData")
load("./Workspaces/top_lda_topics_df.RData")

write.csv(topics_df_lda,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_df_lda.csv")

names(topics_df_lda) <- seq(1:ncol(topics_df_lda))
topics_df_lda$country <- top_topics_lda$country
topics_df_lda

write.csv(topics_df_lda,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_contribution_lda.csv")

most_likely_topics_lda
# 11, 5, 23, 11, 8, 23, 5, 11, 8, 23, 3
top_5_topics_lda <- aggregate(cbind(topics_df_lda$'11', topics_df_lda$'5', topics_df_lda$'23', topics_df_lda$'8', topics_df_lda$'3') ~ country, data=topics_df_lda, FUN=mean)
top_5_topics_lda

save.image("./Workspaces/top_5_lda_topics.RData")

load("./Workspaces/top_5_lda_topics.RData")

# add topic names from earlier 
names(top_5_topics_lda) <- c('Country', '11: President / People / Want / Thank', '5: French-Canadian People / Help / Foreign / Support', '23: National Health Service / Hospital / Health', '8: COVID-19 Cases', '3: Work / Import / Nation / Measures')
top_5_topics_lda

```

### Trying this for DFM 2 
```{r echo = TRUE}
topics_df_lda_2 <- data.frame(t(data.frame(lda_topics_2)))
head(topics_df_lda_2)

save(topics_df_lda_2, file = "./Data/top_topics_lda_df_2.Rdata")
save.image("./Workspaces/top_lda_topics_df_2.RData")
load("./Workspaces/top_lda_topics_df_2.RData")

write.csv(topics_df_lda_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_df_lda_2.csv")


names(topics_df_lda_2) <- seq(1:ncol(topics_df_lda_2))
topics_df_lda_2$country <- top_topics_lda_2$country
topics_df_lda_2

write.csv(topics_df_lda_2,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/topics_contribution_lda_2.csv")
most_likely_topics_lda_2
# 13, 15, 25, 2,24, 5, 15, 13, 24, 12, 14
top_5_topics_lda_2 <- aggregate(cbind(topics_df_lda_2$'13', topics_df_lda_2$'15', topics_df_lda_2$'25', topics_df_lda_2$'2', topics_df_lda_2$'24') ~ country, data=topics_df_lda_2, FUN=mean)
top_5_topics_lda_2

save.image("./Workspaces/top_5_lda_topics_2.RData")

load("./Workspaces/top_5_lda_topics_2.RData")

top_terms_lda_50_2
# add topic names from earlier 
names(top_5_topics_lda_2) <- c('Country', '13: The People', '15: French-Canadian People', '25: Social Distancing', '2: Reopening', '24: COVID-19 Cases')
top_5_topics_lda_2





```



Topic Stability 
```{r topic_stability, echo = TRUE}
# dfm trimmed is the original quanteda dfm without removing numbers 
lda_tm_2 <- LDA(dfm_trimmed, k = 25, method = "Gibbs", iter=3000, control = list(seed = 12345))

save.image("./Workspaces/top_5_lda_topics_2.RData")
load("./Workspaces/top_5_lda_topics_2.RData")

log_likelihood_2 <- lda_tm_2@loglikelihood
log_likelihood_2

similarity_between_lda_models <- simil(lda_tm@beta, lda_tm_2@beta, method = 'cosine')
head(similarity_between_lda_models)

# topic that is the closest match in the original run in terms of cosine similarity of the topic distribution over words
match_max <- apply(similarity_between_lda_models,1, which.max)
match_topics <- as.data.frame(cbind(seq(1:nrow(similarity_between_lda_models)), match_max))
names(match_topics)[1] <- "LDA Topic Model 1"
names(match_topics)[2] <- "LDA Topic Model 2"

match_topics

# num of words in top 10 words shared by each matched topic pair
as.data.frame(top10_words_lda)

# k = 10 for the top 10 words shared by each mateched topic pair
top10_words_lda_2 <- get_terms(lda_tm_2, k=10)
as.data.frame(top10_words_lda_2)

# shared words
# begin a counter
shared_words_num=0
# for loop for all the rows in match_topics table above
for (i in 1:nrow(match_topics)) {
   topic_number = as.numeric(match_topics$`LDA Topic Model 1`[i])
   top10_words_new_1 = top10_words_lda[, topic_number]
   matched_topic_number = as.numeric(match_topics$`LDA Topic Model 2`[i])
   top10_words_new_2 = top10_words_lda_2[, matched_topic_number]
   shared_words_num[i] = length(intersect(top10_words_new_1, top10_words_new_2))
}

match_topics$shared_words_num <- shared_words_num
match_topics$avg_shared_words <- shared_words_num/10
match_topics
```

## Topic Stability for DFM 2 
```{r echo = TRUE}

# dfm trimmed is the newl quanteda dfm without numbers 
lda_tm_2_2 <- LDA(dfm_trimmed_2, k = 25, method = "Gibbs", iter=3000, control = list(seed = 12345))

save.image("./Workspaces/top_5_lda_topics_2_2.RData")
load("./Workspaces/top_5_lda_topics_2_2.RData")

log_likelihood_2_2 <- lda_tm_2_2@loglikelihood
log_likelihood_2_2

similarity_between_lda_models_2 <- simil(lda_tm_2@beta, lda_tm_2_2@beta, method = 'cosine')
head(similarity_between_lda_models_2)

# topic that is the closest match in the original run in terms of cosine similarity of the topic distribution over words
match_max_2 <- apply(similarity_between_lda_models_2,1, which.max)
match_topics_2 <- as.data.frame(cbind(seq(1:nrow(similarity_between_lda_models_2)), match_max_2))
names(match_topics_2)[1] <- "LDA Topic Model 1, DFM 2"
names(match_topics_2)[2] <- "LDA Topic Model 2, DFM 2"

match_topics_2

# num of words in top 10 words shared by each matched topic pair
as.data.frame(top10_words_lda_2_2)

# k = 10 for the top 10 words shared by each mateched topic pair
top10_words_lda_2_2_2 <- get_terms(lda_tm_2_2, k=10)
as.data.frame(top10_words_lda_2_2_2)

# shared words
# begin a counter
shared_words_num_2=0
# for loop for all the rows in match_topics table above
for (i in 1:nrow(match_topics_2)) {
   topic_number = as.numeric(match_topics_2$`LDA Topic Model 1, DFM 2`[i])
   top10_words_new_1 = top10_words_lda_2_2[, topic_number]
   matched_topic_number = as.numeric(match_topics$`LDA Topic Model 2, DFM 2`[i])
   top10_words_new_2 = top10_words_lda_2_2_2[, matched_topic_number]
   shared_words_num[i] = length(intersect(top10_words_new_1, top10_words_new_2))
}

match_topics_2$shared_words_num <- shared_words_num_2
match_topics_2$avg_shared_words <- shared_words_num_2/10
match_topics_2

save.image("./Workspaces/match_topics_DFM2.RData")
load("./Workspaces/match_topics_DFM2.RData")
```


## Topic Models with Covariates

```{r echo = TRUE}



```




```{r stm, echo=TRUE}

system.time(
stm <- stm(poliblog5k.docs, poliblog5k.voc, 3, prevalence = ~rating + s(day), data = poliblog5k.meta))



```

## Webscraping 
s
R Webscraping Code 
```{r scrape_text, echo = TRUE}

# 
# #Parse JSON of urls of all covid related transcripts
# json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
# 
# df <- lapply(json_data, function(url) 
# {
#   data.frame(matrix(unlist(url), ncol = 2, byrow=T))
# })
# 
# df <- do.call(rbind, df)
# 
# colnames(df) <- c('url', 'title')
# rownames(df) <- NULL
# 
# #Correcting URLs
# url_prefix <-'https://www.rev.com/blog/transcripts/'
# titles <- gsub(' ', '-', df$title) %>% tolower()
# titles <- iconv(titles, 'utf-8', 'ascii', sub='')
# titles <- gsub(':', '', titles) 
# df$url <- Map(paste, url_prefix, titles, sep = "")
# 
# #Selecting only transcripts of Country Leaders
# toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.', 'Jacinda', 'Ardern')
# matches <- df[(grepl(paste(toMatch,collapse='|'), 
#                      df$title)), ]
# row.names(matches) <- NULL
# 
# 
# #Special URLs
# matches$url[19] = 'https://www.rev.com/blog/transcripts/donald-trump-coronavirus-press-briefing-transcript-april-16'
# matches$url[69] = 'https://www.rev.com/blog/transcripts/transcript-trump-signs-historic-2-trillion-coronavirus-stimulus-bill'
# matches$url[93] = 'https://www.rev.com/blog/transcripts/justin-trudeau-coronavirus-update-for-canada-march-20'
# matches$url[110] = 'https://www.rev.com/blog/transcripts/donald-trump-mike-pence-and-coronavirus-update-transcript-march-9'
# 
# 
# #Function to scrape text without speaker names
# scrapetextonly <- function(url, title){
#   transcript <- read_html(url)
#   text <- transcript %>% 
#     rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>% 
#     xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>% 
#     rvest::html_text()
#   
#   text_fname = paste(title, '.txt', sep = '')
#   text_fname = paste('Data/all_texts/', text_fname, sep = '')
#   write_file(text_fname, text)
#   return(toString(text))
# }
# 
# write_file <- function(fname, data){
#   fileConn<-file(fname)
#   writeLines(data, fileConn)
#   close(fileConn)  
# }
# 
# 
# #try catch for scraping
# try_scrape <- function(url, title) {
#   out <- tryCatch(
#     {
#       scrapetextonly(url, title)
#     },
#     error=function(cond) {
#       message(paste("URL does not seem to exist:", url))
#       message("Here's the original error message:")
#       message(cond)
#       return(NA)
#     }
#   )    
#   return(out)
# }
# 
# 
# #Scraping the text
# errs <- c()
# texts <- c()
# for( i in 1:nrow(matches)){
#   trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
#   if(is.na(trans)){
#     errs <- append(errs, i)
#   }
#   texts <- append(texts, trans)
# }
# 
# #Adding texts as column to dataframe
# matches$text <- texts[2:121]
# 
# 
# save.image('scraped.RData')
# load('scraped.RData')

```

## Workspace Saving
Saving... 
```{r echo=TRUE}
# save workspace
save.image("./Workspaces/workspace_master.RData")
rm(list = ls())
load("./Workspaces/workspace_master.RData")

```




