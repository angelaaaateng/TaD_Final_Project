---
title: "COVID_TopicModel"
author: "Angela Teng"
date: "4/29/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Text as Data Final Project 
Lakshmi Menon, Angela Marie Teng

```{r include = FALSE}
# get current working directory
rm(list = ls())
getwd()  
# setwd("~/Data/articles")

# loading packages 
library(dplyr)
library(ggplot2)
library(xtable)
library(plyr)

library(rvest)
library(dplyr)

# Loading multiple packages
libraries <- c("foreign", "stargazer")
lapply(libraries, require, character.only=TRUE)
set.seed(100)
library(quanteda)
library("devtools")
# install.packages("textreadr")
library(textreadr)

library(rjson)

# Load it into our environment
library(quanteda.corpora)
library(quanteda)
# install.packages("tm")
library(tm)
library(readtext)
# install.packages("spacyr")
library(spacyr)

libraries <- c("topicmodels", "dplyr", "stm", "quanteda")
lapply(libraries, require, character.only = T)
# to check version
packageVersion("quanteda")
# install.packages("magicfor")
library(magicfor)
```

Loading in all the articles from rev.com:
```{r data_load, echo =TRUE}
# temp = list.files(pattern="*.txt")
# myfiles = lapply(temp, read.delim)

## the read in of a directory
data = read_dir('/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/articles')

summary(data)
typeof(data)
attributes(data)
```

Selecting only articles that contain keywords per country: 
UK - UK, [Boris] Johnson
US - US, Trump 
New Zealand - New Zealand, [Jacinda] Ardern
Australia - Australia, [Scott] Morrison
Canada - Canada, [Justin] Trudeau

(this data is from Data/Articles folder)

Load in scraped data: 

```{r data_scraped, echo = TRUE}
load('./Workspaces/scraped_dates.RData')
summary(matches)

matches$country <- ifelse(grepl("Donald|Trump|US|United States", matches$title), "US", "country")

matches$country <- ifelse(grepl("Donald|Trump|US|United States", matches$title), "US", matches$country)

matches$country <- ifelse(grepl("Boris|Johnson|United Kingdom|UK", matches$title), "UK", matches$country)

matches$country <- ifelse(grepl("Jacinda|Ardern|New Zealand", matches$title), "NZ", matches$country)

matches$country <- ifelse(grepl("Scott|Morrison|Australia", matches$title), "AU", matches$country)

matches$country <- ifelse(grepl("Justin|Trudeau|Canada", matches$title), "CA", matches$country)

head(matches)

colnames(matches)
typeof(matches)
attributes(matches)

```

```{r subset_data, echo=TRUE}
matches[,grepl("trump", colnames("title"))]

# turn list into df
df <- data.frame(matches)
# sanity check
df[1,1:2]

```

Now, we want to preprocess our text and convert it into a dfm
```{r preprocessing, echo = TRUE}

# turn list into df
df <- as.data.frame(matches, stringsAsFactors=F)

attributes(df)

typeof(df)
typeof(df$text)

mode(df$text)
storage.mode(df$text) <- "character"

covid_corp <- corpus(df, text_field ="text" )

df <- apply(df,2,as.character)
head(df)

write.csv(df,"/Users/angelateng/Desktop/Github_Files_Teng/TaD_Final_Project/Data/master.csv")


head(covid_corp)
# a corpus consists ofs: (1) documents: text + doc level data (2) corpus metadata (3) extras (settings)
head(docvars(covid_corp))  # document-level variables
metacorpus(covid_corp)  # corpus-level variables

# ndoc identifies the number of documents in a corpus
ndocs <- ndoc(covid_corp)
# this is because it's subdividedd into paragraph, rather than article.....
ndocs

corpusinfo <- summary(covid_corp, n = ndocs)  # note n default is 100
head(corpusinfo)
```



```{r echo = TRUE}
# STM

# data <- read.csv("./Data/master.csv")
# head(data)
length(covid_corp$documents)
processed <- textProcessor(covid_corp$documents, metadata = covid_corp)


```


```{r echo = TRUE}
system.time(
blog_stm <- stm(docvars(covid_corp), 3, prevalence = ~rating + s(day), data = poliblog5k.meta))




# Using data from the corpus
us_tokens_1 <- tokens(US_briefing_apr1, remove_punct = TRUE, stem = TRUE, remove_numbers = FALSE, remove_symbols = TRUE, remove_url = TRUE, tolower=TRUE, remove_hyphens=TRUE) 

# tokens
num_tokens_us_1 <- sum(lengths(us_tokens_1))
num_tokens_us_1

us_dfm_apr1 <- dfm(us_corp, tolower=TRUE, stem = TRUE,  remove = stopwords("english"), remove_punct = TRUE)
us_dfm_apr1

# write a function do to this for all trump documents 
US_uniq_docs[1]


new_dfm <- function(input_corpus) {
  dfm(input_corpus, tolower=TRUE, stem = TRUE,  remove = stopwords("english"), remove_punct = TRUE)
}
  
new_dfm(US_uniq_docs[1])


```


Now that we have a DFM, we want to do basic exploratory data analysis
```{r eda, echo=TRUE}


```


R Webscraping Code 
```{r scrape_text, echo = TRUE}

# 
# #Parse JSON of urls of all covid related transcripts
# json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
# 
# df <- lapply(json_data, function(url) 
# {
#   data.frame(matrix(unlist(url), ncol = 2, byrow=T))
# })
# 
# df <- do.call(rbind, df)
# 
# colnames(df) <- c('url', 'title')
# rownames(df) <- NULL
# 
# #Correcting URLs
# url_prefix <-'https://www.rev.com/blog/transcripts/'
# titles <- gsub(' ', '-', df$title) %>% tolower()
# titles <- iconv(titles, 'utf-8', 'ascii', sub='')
# titles <- gsub(':', '', titles) 
# df$url <- Map(paste, url_prefix, titles, sep = "")
# 
# #Selecting only transcripts of Country Leaders
# toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.', 'Jacinda', 'Ardern')
# matches <- df[(grepl(paste(toMatch,collapse='|'), 
#                      df$title)), ]
# row.names(matches) <- NULL
# 
# 
# #Special URLs
# matches$url[19] = 'https://www.rev.com/blog/transcripts/donald-trump-coronavirus-press-briefing-transcript-april-16'
# matches$url[69] = 'https://www.rev.com/blog/transcripts/transcript-trump-signs-historic-2-trillion-coronavirus-stimulus-bill'
# matches$url[93] = 'https://www.rev.com/blog/transcripts/justin-trudeau-coronavirus-update-for-canada-march-20'
# matches$url[110] = 'https://www.rev.com/blog/transcripts/donald-trump-mike-pence-and-coronavirus-update-transcript-march-9'
# 
# 
# #Function to scrape text without speaker names
# scrapetextonly <- function(url, title){
#   transcript <- read_html(url)
#   text <- transcript %>% 
#     rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>% 
#     xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>% 
#     rvest::html_text()
#   
#   text_fname = paste(title, '.txt', sep = '')
#   text_fname = paste('Data/all_texts/', text_fname, sep = '')
#   write_file(text_fname, text)
#   return(toString(text))
# }
# 
# write_file <- function(fname, data){
#   fileConn<-file(fname)
#   writeLines(data, fileConn)
#   close(fileConn)  
# }
# 
# 
# #try catch for scraping
# try_scrape <- function(url, title) {
#   out <- tryCatch(
#     {
#       scrapetextonly(url, title)
#     },
#     error=function(cond) {
#       message(paste("URL does not seem to exist:", url))
#       message("Here's the original error message:")
#       message(cond)
#       return(NA)
#     }
#   )    
#   return(out)
# }
# 
# 
# #Scraping the text
# errs <- c()
# texts <- c()
# for( i in 1:nrow(matches)){
#   trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
#   if(is.na(trans)){
#     errs <- append(errs, i)
#   }
#   texts <- append(texts, trans)
# }
# 
# #Adding texts as column to dataframe
# matches$text <- texts[2:121]
# 
# 
# save.image('scraped.RData')
# load('scraped.RData')

```


Saving... 
```{r echo=TRUE}
# save workspace
save.image("./Workspaces/workspace_master.RData")


rm(list = ls())

load("./workspace_project.RData")

```




