out <- tryCatch(
{
scrapetextonly(url, title)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
# Choose a return value in case of error
return(NA)
}
)
return(out)
}
test <- try_scrape(matches$url[19], matches$title[19])
test <- try_scrape(toString(matches$url[19]), toString(matches$title[19]))
for( i in 19:22){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
texts <- append(texts, trans)
}
errs <- c(19)
for( i in 23:40){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
for( i in 41:60){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
for( i in 61:80){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
for( i in 81:100){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
for( i in 101:120){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
length(errs)
errs
matches$url[54]
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- gsub(':', '', df$title)
titles <- gsub('\" ', '', df$title)
titles[54]
df$url <- Map(paste, url_prefix, titles, sep = "")
View(df)
View(df)
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- gsub(':', '', titles)
titles <- gsub('\" ', '', titles)
df$url <- Map(paste, url_prefix, titles, sep = "")
View(df)
titles <- gsub('\' ', '', titles)
df$url <- Map(paste, url_prefix, titles, sep = "")
View(df)
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- gsub(':', '', titles)
titles <- gsub('‘', '', titles)
titles <- gsub('\’', '', titles)
df$url <- Map(paste, url_prefix, titles, sep = "")
df$url[248]
combined_doc <- iconv(df$title[248], 'utf-8', 'ascii', sub='')
combined_doc
titles <- iconv(df$title, 'utf-8', 'ascii', sub='')
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- gsub(':', '', titles)
titles <- gsub('‘', '', titles)
titles <- gsub('\’', '', titles)
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
df <- lapply(json_data, function(url)
{
data.frame(matrix(unlist(url), ncol = 2, byrow=T))
})
df <- do.call(rbind, df)
colnames(df) <- c('url', 'title')
rownames(df) <- NULL
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- iconv(df$title, 'utf-8', 'ascii', sub='')
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- gsub(':', '', titles)
df$url <- Map(paste, url_prefix, titles, sep = "")
df$url[248]
titles[248]
df <- lapply(json_data, function(url)
{
data.frame(matrix(unlist(url), ncol = 2, byrow=T))
})
df <- do.call(rbind, df)
colnames(df) <- c('url', 'title')
rownames(df) <- NULL
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
titles[248]
titles <- iconv(titles, 'utf-8', 'ascii', sub='')
titles[248]
titles <- gsub(':', '', titles)
titles[248]
df$url <- Map(paste, url_prefix, titles, sep = "")
df$url[248]
rm(list = ls())
library(rvest)
library(dplyr)
library(rjson)
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Project/TaD_Final_Project")
#Parse JSON of urls of all covid related transcripts
json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
df <- lapply(json_data, function(url)
{
data.frame(matrix(unlist(url), ncol = 2, byrow=T))
})
df <- do.call(rbind, df)
colnames(df) <- c('url', 'title')
rownames(df) <- NULL
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- iconv(titles, 'utf-8', 'ascii', sub='')
titles <- gsub(':', '', titles)
df$url <- Map(paste, url_prefix, titles, sep = "")
#Selecting only transcripts of Country Leaders
toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.', 'Jacinda', 'Ardern')
matches <- df[(grepl(paste(toMatch,collapse='|'),
df$title)), ]
row.names(matches) <- NULL
try_scrape <- function(url, title) {
out <- tryCatch(
{
scrapetextonly(url, title)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
for( i in c(19,54){
for( i in c(19,54)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
errs <- c()
errs <- c()
for( i in c()){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
errs <- c()
for( i in c(19, 54)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
scrapetextonly <- function(url){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
return(toString(text))
}
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
print('Saved')
close(fileConn)
}
try_scrape <- function(url, title) {
out <- tryCatch(
{
scrapetextonly(url, title)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
errs <- c()
for( i in c(19, 54)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
scrapetextonly <- function(url, title){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
text_fname = paste(title, '.txt', sep = '_')
write_file(text_fname, text)
return(toString(text))
}
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
print('Saved')
close(fileConn)
}
try_scrape <- function(url, title) {
out <- tryCatch(
{
scrapetextonly(url, title)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
errs <- c()
for( i in c(19, 54)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
scrapetextonly <- function(url, title){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
text_fname = paste(title, '.txt', sep = '_')
text_fname = paste('Data/new_texts', text_fname, sep = '_')
write_file(text_fname, text)
return(toString(text))
}
rm(list = ls())
library(rvest)
library(dplyr)
library(rjson)
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Project/TaD_Final_Project")
#Parse JSON of urls of all covid related transcripts
json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
df <- lapply(json_data, function(url)
{
data.frame(matrix(unlist(url), ncol = 2, byrow=T))
})
df <- do.call(rbind, df)
colnames(df) <- c('url', 'title')
rownames(df) <- NULL
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- iconv(titles, 'utf-8', 'ascii', sub='')
titles <- gsub(':', '', titles)
df$url <- Map(paste, url_prefix, titles, sep = "")
#Selecting only transcripts of Country Leaders
toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.', 'Jacinda', 'Ardern')
matches <- df[(grepl(paste(toMatch,collapse='|'),
df$title)), ]
row.names(matches) <- NULL
scrapetextonly <- function(url, title){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
text_fname = paste(title, '.txt', sep = '_')
text_fname = paste('Data/new_texts', text_fname, sep = '_')
write_file(text_fname, text)
return(toString(text))
}
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
close(fileConn)
}
try_scrape <- function(url, title) {
out <- tryCatch(
{
scrapetextonly(url, title)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
errs <- c()
for( i in 1:20){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
scrapetextonly <- function(url, title){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
text_fname = paste(title, '.txt', sep = '')
text_fname = paste('Data/new_texts/', text_fname, sep = '')
write_file(text_fname, text)
return(toString(text))
}
for( i in 1:20){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
errs <- c()
for( i in 1:nrow(matches)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
write_file(text_fname, 'hi')
text_fname = paste('Data/new_texts/', 'test', sep = '')
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
close(fileConn)
}
write_file(text_fname, 'hi')
title = 'test'
text_fname = paste(title, '.txt', sep = '')
text_fname = paste('Data/new_texts/', text_fname, sep = '')
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
close(fileConn)
}
write_file(text_fname, 'hi')
title = 'test'
text_fname = paste(title, '.txt', sep = '')
text_fname = paste('Data/all_texts/', text_fname, sep = '')
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
close(fileConn)
}
write_file(text_fname, 'hi')
View(texts)
View(texts)
texts[[7]]
rm(list = ls())
library(rvest)
library(dplyr)
library(rjson)
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Project/TaD_Final_Project")
#Parse JSON of urls of all covid related transcripts
json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
df <- lapply(json_data, function(url)
{
data.frame(matrix(unlist(url), ncol = 2, byrow=T))
})
df <- do.call(rbind, df)
colnames(df) <- c('url', 'title')
rownames(df) <- NULL
#Correcting URLs
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
titles <- iconv(titles, 'utf-8', 'ascii', sub='')
titles <- gsub(':', '', titles)
df$url <- Map(paste, url_prefix, titles, sep = "")
#Selecting only transcripts of Country Leaders
toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.', 'Jacinda', 'Ardern')
matches <- df[(grepl(paste(toMatch,collapse='|'),
df$title)), ]
row.names(matches) <- NULL
scrapetextonly <- function(url, title){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
text_fname = paste(title, '.txt', sep = '')
text_fname = paste('Data/all_texts/', text_fname, sep = '')
write_file(text_fname, text)
return(toString(text))
}
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
close(fileConn)
}
try_scrape <- function(url, title) {
out <- tryCatch(
{
scrapetextonly(url, title)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
errs <- c()
for( i in 1:nrow(matches)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs <- append(errs, i)
}
texts <- append(texts, trans)
}
save.image('scraped_most.RData')
load('scraped_most.RData')
save.image('scraped_most.RData')
load('scraped_most.RData')
for( i in c(19, 69, 93, 110)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs2 <- append(errs, i)
}
texts[i] <- trans
}
#Special URLs
matches$url[19] = 'https://www.rev.com/blog/transcripts/donald-trump-coronavirus-press-briefing-transcript-april-16'
matches$url[69] = 'https://www.rev.com/blog/transcripts/transcript-trump-signs-historic-2-trillion-coronavirus-stimulus-bill'
matches$url[93] = 'https://www.rev.com/blog/transcripts/justin-trudeau-coronavirus-update-for-canada-march-20'
matches$url[110] = 'https://www.rev.com/blog/transcripts/donald-trump-mike-pence-and-coronavirus-update-transcript-march-9'
errs2 = c()
for( i in c(19, 69, 93, 110)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs2 <- append(errs, i)
}
texts[i] <- trans
}
View(texts)
View(texts)
load('scraped_most.RData')
View(texts)
View(texts)
#Special URLs
matches$url[19] = 'https://www.rev.com/blog/transcripts/donald-trump-coronavirus-press-briefing-transcript-april-16'
matches$url[69] = 'https://www.rev.com/blog/transcripts/transcript-trump-signs-historic-2-trillion-coronavirus-stimulus-bill'
matches$url[93] = 'https://www.rev.com/blog/transcripts/justin-trudeau-coronavirus-update-for-canada-march-20'
matches$url[110] = 'https://www.rev.com/blog/transcripts/donald-trump-mike-pence-and-coronavirus-update-transcript-march-9'
errs2 = c()
for( i in c(19, 69, 93, 110)){
trans <- try_scrape(toString(matches$url[i]), toString(matches$title[i]))
if(is.na(trans)){
errs2 <- append(errs, i)
}
texts[i+1] <- trans
}
View(texts)
View(texts)
text[19]
text[[19]]
texts[19]
texts[20]
View(texts)
View(texts)
View(matches)
length(texts)
matches$text <- texts[2:121]
View(matches)
save.image('scraped.RData')
View(matches)
save.image('scraped.RData')
