View(data("data_corpus_amicus"))
knitr::opts_chunk$set(echo = TRUE)
reviews <- read.csv(file = 'yelp.csv')
reviews <- read.csv(file = 'yelp.csv')
median_stars <- median(reviews$stars)
median_stars <- median(reviews$stars)
reviews$label <- ifelse(reviews$stars>=4, 'positive', 'negative')
reviews$label <- ifelse(reviews$stars>=4, 'positive', 'negative')
reviews_short <- reviews[1:1000,]
reviews_short$text <- gsub(pattern = "'", "", reviews_short$text)  # replace apostrophes
prop.table(table(reviews_short$label))
reviews_short <- reviews_short %>% sample_n(nrow(reviews_short))
library(dplyr)
reviews_short <- reviews_short %>% sample_n(nrow(reviews_short))
rownames(reviews_short) <- NULL
reviews_dfm <- dfm(reviews_short$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english")) %>% convert("matrix")
library(quanteda)
library(quanteda)
library(quanteda.corpora)
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
reviews_dfm <- dfm(reviews_short$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english")) %>% convert("matrix")
run_svm <- function(prop, method = 'svmLinear'){
ids_train <- createDataPartition(1:nrow(reviews_dfm), p = prop, list = FALSE, times = 1)
train_x <- reviews_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- reviews_short$label[ids_train] %>% as.factor()  # train set labels
val_x <- reviews_dfm[-ids_train, ]  %>% as.data.frame() # test set data
val_y <- reviews_short$label[-ids_train] %>% as.factor() # test set labels
trctrl <- trainControl(method = "cv",
number = 5)
svm_mod <- train(x = train_x,
y = train_y,
method = method,
trControl = trctrl)
svm_pred <- predict(svm_mod, newdata = val_x)
svm_cmat <- confusionMatrix(svm_pred, val_y, positive = 'positive')
#print(svm_cmat)
return(svm_cmat$overall[["Accuracy"]])
}
acc_20 <- run_svm(0.2)
acc_20 <- run_svm(0.2)
acc_50 <- run_svm(0.5)
acc_70 <- run_svm(0.7)
cat('\n',
"SVM-Linear Accuracy 20%:",  acc_20, '\n',
"SVM-Linear Accuracy 50%:",  acc_50, '\n',
"SVM-Linear Accuracy 70%:",  acc_70, '\n'
)
acc_radial <- run_svm(0.7, 'svmRadial')
acc_radial <- run_svm(0.7, 'svmRadial')
cat("\n SVM-Linear Accuracy 50%:",  acc_radial, '\n')
library(text2vec)
knitr::opts_chunk$set(echo = TRUE)
# import libraries
rm(list = ls())
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Homework/Homework3")
libraries <- c("dplyr", "quanteda", "quanteda.corpora", "ldatuning", "topicmodels",
"ggplot2", "stm", "tm", "factoextra", "text2vec", "lsa", "bursts")
lapply(libraries, require, character.only = T)
# import libraries
rm(list = ls())
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Homework/Homework3")
libraries <- c("dplyr", "quanteda", "quanteda.corpora", "ldatuning", "topicmodels",
"ggplot2", "stm", "tm", "factoextra", "text2vec", "lsa", "bursts")
lapply(libraries, require, character.only = T)
news <- corpus_subset(data_corpus_immigrationnews,
paperName %in% c("telegraph", "guardian", "ft", "independent", "express"))
table(news$documents$paperName)
load('custom_stopwords.RData')
news_dfm <- dfm(news, remove = custom_stopwords, remove_punct = TRUE, stem = TRUE, tolower = TRUE, remove_numbers = TRUE)
news_dfm <- dfm_trim(news_dfm, min_termfreq = 30, min_docfreq = 20)
news_dfm
##### MODEL CODE #####
#news_tm <- LDA(news_dfm, k = 25, method = "Gibbs",  control = list(seed = 1234), iter = 3000)
#saveRDS(news_tm, 'news_tm')
#save.image('myworkspace.RData')
load('myworkspace.RData')
news_tm@loglikelihood
top_terms <- get_terms(news_tm, 10)
print(top_terms)
likely_topic <- topics(news_tm)
topics <- data.frame(table(likely_topic))
topics <- topics[order(topics$Freq, decreasing = TRUE),]
row.names(topics) <- NULL
print(topics)
top_5 <- c(topics$likely_topic[1:5])
for(topic in top_5){
top <- paste('Topic', topic)
print(top)
print(top_terms[,top])
}
which.max2 <- function(x){
which(x == sort(x,partial=(25-1))[25-1])
}
plot_top2 <-function(name){
ft <- which(news$documents$paperName == name)
first <- ft[1]
last <- tail(ft, n=1)
doc_topics <- news_tm@gamma[first:last,]
days <- as.numeric(news$documents$day[first:last])
doc_topics <- t(doc_topics)
max <- apply(doc_topics, 2, which.max)
max2 <- apply(doc_topics, 2, which.max2)
max2 <- sapply(max2, max)
top2 <- data.frame(top_topic = max, second_topic = max2, date = days)
top2 <- top2[order(top2$date),]
title = paste("Top 2 Topics for", name)
ft_plot <- ggplot(top2, aes(x=date, y=top_topic, pch="First"))
ft_plot + geom_point(aes(x=date, y=second_topic, pch="Second") ) +theme_bw() +
ylab("Topic Number") + ggtitle(title) + geom_point() + xlab(NULL) +
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
}
plot_top2('ft')
plot_top2('guardian')
names = c("telegraph", "guardian", "ft", "independent", "express")
data = c()
i=1
for(name in names){
vals <- which(news$documents$paperName == name)
first <- vals[1]
last <- tail(vals, n=1)
mean_vals = c()
for(topic in top_5){
doc_topics <- news_tm@gamma[first:last, topic]
mean_vals <- append(mean_vals, mean(doc_topics))
}
data[[i]] = mean_vals
i = i+1
}
tab <- as.data.frame(data, row.names = names, col.names = top_5)
print(tab)
#news_tm_2 <- LDA(news_dfm, k = 25, method = "Gibbs",  control = list(seed = 2000), iter = 3000)
#saveRDS(news_tm_2, 'news_tm_q2a')
#save.image('myworkspace_q2.RData')
load('myworkspace_q2.RData')
news_tm_2@loglikelihood
q2 <- function(tm1, tm2){
dist_1 <- tm1@beta
dist_2 <- tm2@beta
data2 = c()
for(row2 in 1:nrow(dist_2)){
sims <- c()
for(row1 in 1:nrow(dist_1)){
cosine_val <- cosine(dist_2[row2, ], dist_1[row1,])
sims <- append(sims, cosine_val)
}
data2[[row2]] = (which.max(sims))
}
cosine_sim <- data.frame(data2)
colnames(cosine_sim) <- 'Model1_Topic'
cosine_sim$'Model2_Topic' <- rownames(cosine_sim)
cosine_sim
top_terms1 <- get_terms(tm1, 10)
top_terms2 <- get_terms(tm2, 10)
commons<- c()
for(topic in 1:nrow(cosine_sim)){
list2 <- top_terms2[, topic]
list1 <- top_terms1[, cosine_sim[topic,1] ]
commons <- append(commons, (length(intersect(list1, list2))))
}
print(cosine_sim)
cosine_sim$common <- commons
return(cosine_sim)
}
sim_table <- q2(news_tm, news_tm_2)
print(sim_table)
cat('Average Common Words', mean(sim_table$common))
##### MODEL CODE #####
#news_tm_5_1234 <- LDA(news_dfm, k = 5, method = "Gibbs",  control = list(seed = 1234), iter = 3000)
#news_tm_5_2000 <- LDA(news_dfm, k = 5, method = "Gibbs",  control = list(seed = 2000), iter = 3000)
#saveRDS(news_tm_5_1234, 'news_tm_5_1234')
#saveRDS(news_tm_5_2000, 'news_tm_5_2000')
#save.image('myworkspace_q2d.RData')
load('myworkspace_q2d.RData')
news_tm_5_1234@loglikelihood
news_tm_5_2000@loglikelihood
sim_table2 <- q2(news_tm_5_1234, news_tm_5_2000)
cat('Average Common Words', mean(sim_table2$common))
##### MODEL CODE #####
#news_stm_content <- stm(news_dfm, K=0, prevalence = ~paperName + s(date), content = ~paperName, init.type = 'Spectral', seed = 2000)
#saveRDS(news_stm_content, 'news_stm_content')
#save.image('myworkspace_q3.RData')
load('myworkspace_q3.RData')
plot(news_stm_content, type = 'summary')
plot(news_stm_content, type="perspectives", topics = 49)
prep <- estimateEffect(c(49) ~paperName + s(date) , news_stm_content, metadata = news$documents)
plot(prep, "date", news_stm_content, topics = 49,
method = "continuous", xaxt = "n", xlab = "Date")
data <- corpus_subset(data_corpus_ukmanifestos, Party %in% c('Con', 'Lab'))
lab_con_dfm <- dfm(data,
stem = T,
remove = stopwords("english"),
remove_punct = T)
lab_index <- which(data$documents$Year == 1979 & data$documents$Party == 'Lab' )
con_index <- which(data$documents$Year == 1979 & data$documents$Party == 'Con' )
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(lab_index, con_index)) # second parameter corresponds to index texts
textplot_scale1d(manifestos_fish, groups = data$party)
summary(manifestos_fish)
words <- manifestos_fish$psi
weights <- manifestos_fish$beta
plot(weights, words)
x <- manifestos_fish$theta
y <- ifelse(data$documents$Party == 'Lab', 1, 0)
lin_model <- lm(y ~ x, data = lab_con_dfm)
summary(lin_model)
## From Lab 13
bursty <- function(word, DTM, date) {
word.vec <- DTM[, which(colnames(DTM) == word)]
if(length(word.vec) == 0) {
print(paste(word, " does not exist in this corpus."))
return()
}
else {
word.times <- c(0,which(as.vector(word.vec)>0))
kl <- kleinberg(word.times, gamma = 0.5)
kl$start <- date[kl$start+1]
kl$end <- date[kl$end]
max_level <- max(kl$level)
plot(c(kl$start[1], kl$end[1]), c(1,max_level),
type = "n", xlab = "Time", ylab = "Level", bty = "n",
xlim = c(min(date), max(date)), ylim = c(1, max_level),
yaxt = "n")
axis(2, at = 1:max_level)
for (i in 1:nrow(kl)) {
if (kl$start[i] != kl$end[i]) {
arrows(kl$start[i], kl$level[i], kl$end[i], kl$level[i], code = 3, angle = 90,
length = 0.05)
}
else {
points(kl$start[i], kl$level[i])
}
}
print(kl)
}
}
news <- readRDS('news_data.rds')
news_corpus <- corpus(news$headline)
news_dfm <- dfm(news_corpus)
bursty("obama", news_dfm, as.Date(news$date))
bursty("korea", news_dfm, as.Date(news$date))
bursty("afghanistan", news_dfm, as.Date(news$date))
news_data <- readRDS("news_data.rds")
table(news_data$category)
set.seed(2000)
news_samp <- news_data %>%
filter(category %in% c("WORLD NEWS")) %>%
group_by(category) %>%
sample_n(1000) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
news_dfm <- dfm(news_samp$text, tolower = TRUE, remove_punct = TRUE, remove = stopwords("english"))
news_mat <- convert(news_dfm, to = "matrix")
news_pca <- prcomp(news_mat, center = TRUE, scale = TRUE)
pc_loadings <- news_pca$rotation
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, 5, loading),top_n(pc1_loading, -5, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
pc1_loading
news_mat_lsa <- convert(news_dfm, to = "lsa") # convert to transposed matrix (so terms are rows and columns are documents = TDM)
news_mat_lsa <- lw_logtf(news_mat_lsa) * gw_idf(news_mat_lsa) # local - global weighting (akin to TFIDF)
news_lsa <- lsa(news_mat_lsa)
news_lsa_mat <- as.textmatrix(news_lsa)
america <- associate(news_lsa_mat, "america", "cosine", threshold = .7)
america[1:5]
corruption <- associate(news_lsa_mat, "corruption", "cosine", threshold = .6)
corruption[1:5]
pretrained <- readRDS("glove.rds") # GloVe pretrained (https://nlp.stanford.edu/projects/glove/)
# function to compute nearest neighbors
nearest_neighbors <- function(cue, embeds, N = 5, norm = "l2"){
cos_sim <- sim2(x = embeds, y = embeds[cue, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # cue is always the nearest neighbor hence dropped
}
america_glove <- nearest_neighbors("state", pretrained, N = 5, norm = "l2")
america_glove
corruption_glove <- nearest_neighbors("welfare", pretrained, N = 5, norm = "l2")
corruption_glove
rm(list = ls())
library(rvest)
library(dplyr)
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Project/TaD_Final_Project")
load('scraped.RData')
scrapetextonly <- function(url, title){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-rich-text')]") %>%
xml2::xml_find_all("div/p/text()") %>%
rvest::html_text()
#text_fname = paste(title, '.txt', sep = '')
#text_fname = paste('Data/all_texts/', text_fname, sep = '')
#write_file(text_fname, text)
#return(toString(text))
}
scrapedate <- function(url){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-rich-text')]") %>%
xml2::xml_find_all("div/p/text()") %>%
rvest::html_text()
#text_fname = paste(title, '.txt', sep = '')
#text_fname = paste('Data/all_texts/', text_fname, sep = '')
#write_file(text_fname, text)
#return(toString(text))
}
#try catch for scraping
try_scrape <- function(url) {
out <- tryCatch(
{
scrapedate(url)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
dates <- c()
for( i in 1:3){
dt <- try_scrape(toString(matches$url[i]))
if(is.na(dt)){
errs <- append(errs, i)
}
dates <- append(dates, dt)
}
scrapedate <- function(url){
transcript <- read_html(url)
date <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-rich-text')]") %>%
xml2::xml_find_all("div/p/text()") %>%
rvest::html_text()
#text_fname = paste(title, '.txt', sep = '')
#text_fname = paste('Data/all_texts/', text_fname, sep = '')
#write_file(text_fname, text)
return(toString(date))
}
#try catch for scraping
try_scrape <- function(url) {
out <- tryCatch(
{
scrapedate(url)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
dates <- c()
for( i in 1:3){
dt <- try_scrape(toString(matches$url[i]))
if(is.na(dt)){
errs <- append(errs, i)
}
dates <- append(dates, dt)
}
scrapedate <- function(url){
transcript <- read_html(url)
date <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-rich-text')]") %>%
xml2::xml_find_all("div/p/text()") %>%
rvest::html_text()
#text_fname = paste(title, '.txt', sep = '')
#text_fname = paste('Data/all_texts/', text_fname, sep = '')
#write_file(text_fname, text)
print(date)
return(toString(date))
}
errs <- c()
dates <- c()
for( i= 1:1){
dt <- try_scrape(toString(matches$url[i]))
if(is.na(dt)){
errs <- append(errs, i)
}
dates <- append(dates, dt)
}
matches$url[1]
scrapedate <- function(url){
transcript <- read_html(url)
date <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-rich-text')]") %>%
xml2::xml_find_all("div/p") %>%
rvest::html_text()
#text_fname = paste(title, '.txt', sep = '')
#text_fname = paste('Data/all_texts/', text_fname, sep = '')
#write_file(text_fname, text)
print(date)
return(toString(date))
}
for( i in 1:1){
dt <- try_scrape(toString(matches$url[i]))
if(is.na(dt)){
errs <- append(errs, i)
}
dates <- append(dates, dt)
}
dates <- c()
scrapedate <- function(url){
transcript <- read_html(url)
date <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-rich-text')]") %>%
xml2::xml_find_all() %>%
rvest::html_text()
#text_fname = paste(title, '.txt', sep = '')
#text_fname = paste('Data/all_texts/', text_fname, sep = '')
#write_file(text_fname, text)
print(date)
return(toString(date))
}
for( i in 1:1){
dt <- try_scrape(toString(matches$url[i]))
if(is.na(dt)){
errs <- append(errs, i)
}
dates <- append(dates, dt)
}
url = 'https://www.rev.com/blog/transcripts/donald-trump-coronavirus-press-conference-transcript-april-23'
transcript <- read_html(url)
View(transcript)
date <- transcript %>%
rvest::html_nodes('body') %>%
xml2::xml_find_all("//div[contains(@class, 'fl-rich-text')]") %>%
rvest::html_text()
date
date <- transcript %>%
rvest::html_nodes('body') %>%
xml2::xml_find_all("//div[contains(@class, 'fl-rich-text')]/p") %>%
rvest::html_text()
date <- transcript %>%
rvest::html_nodes('body') %>%
xml2::xml_find_all("//div[contains(@class, 'fl-rich-text')]/p") %>%
rvest::html_text()
date
date <- transcript %>%
rvest::html_nodes('body') %>%
xml2::xml_find("//div[contains(@class, 'fl-rich-text')]") %>%
rvest::html_text()
date <- transcript %>%
rvest::html_nodes('body') %>%
xml2::xml_find_first("//div[contains(@class, 'fl-rich-text')]") %>%
rvest::html_text()
type(date)
typeof(date)
scrapedate <- function(url){
transcript <- read_html(url)
date <- transcript %>%
rvest::html_nodes('body') %>%
xml2::xml_find_first("//div[contains(@class, 'fl-rich-text')]") %>%
rvest::html_text()
#text_fname = paste(title, '.txt', sep = '')
#text_fname = paste('Data/all_texts/', text_fname, sep = '')
#write_file(text_fname, text)
return(date)
}
#try catch for scraping
try_scrape <- function(url) {
out <- tryCatch(
{
scrapedate(url)
},
error=function(cond) {
message(paste("URL does not seem to exist:", url))
message("Here's the original error message:")
message(cond)
return(NA)
}
)
return(out)
}
errs <- c()
dates <- c()
for( i in 1:nrow(matches)){
dt <- try_scrape(toString(matches$url[i]))
if(is.na(dt)){
errs <- append(errs, i)
}
dates <- append(dates, dt)
}
save.image('scraped_dates.RData')
dates
matches$date <- dates[1:120]
View(matches)
save.image('scraped_dates.RData')
saveRDS(matches, 'final_data')
save.image('scraped_dates.RData')
load('scraped_dates.RData')
rm(date, dt, errs, errs2, trans, url, url_prefix)
rm(i)
save.image('scraped_dates.RData')
getwd()
load('Data/final_data')
#save.image('scraped_dates.RData')
#saveRDS(matches, 'final_data')
write.table(matches, 'final_dataset')
load('Workspaces/scraped_dates.RData')
tm(list = ls)
rm(list = ls)
rm(list = ls())
load('Workspaces/scraped_dates.RData')
matches$date
