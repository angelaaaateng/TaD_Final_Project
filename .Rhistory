#----------------------------------------
# Loading data
data("data_corpus_amicus")
# load required libraries
library(quanteda)
library(quanteda.corpora)
#----------------------------------------
# 3 Applying Naive Bayes and Word Scores to Amicus texts from Evans et al ---
#----------------------------------------
# Loading data
data("data_corpus_amicus")
#----------------------------------------
# 3 Applying Naive Bayes and Word Scores to Amicus texts from Evans et al ---
#----------------------------------------
# Loading data
View(data("data_corpus_amicus"))
knitr::opts_chunk$set(echo = TRUE)
reviews <- read.csv(file = 'yelp.csv')
reviews <- read.csv(file = 'yelp.csv')
median_stars <- median(reviews$stars)
median_stars <- median(reviews$stars)
reviews$label <- ifelse(reviews$stars>=4, 'positive', 'negative')
reviews$label <- ifelse(reviews$stars>=4, 'positive', 'negative')
reviews_short <- reviews[1:1000,]
reviews_short$text <- gsub(pattern = "'", "", reviews_short$text)  # replace apostrophes
prop.table(table(reviews_short$label))
reviews_short <- reviews_short %>% sample_n(nrow(reviews_short))
library(dplyr)
reviews_short <- reviews_short %>% sample_n(nrow(reviews_short))
rownames(reviews_short) <- NULL
reviews_dfm <- dfm(reviews_short$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english")) %>% convert("matrix")
library(quanteda)
library(quanteda)
library(quanteda.corpora)
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
reviews_dfm <- dfm(reviews_short$text, stem = TRUE, remove_punct = TRUE,
remove = stopwords("english")) %>% convert("matrix")
run_svm <- function(prop, method = 'svmLinear'){
ids_train <- createDataPartition(1:nrow(reviews_dfm), p = prop, list = FALSE, times = 1)
train_x <- reviews_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- reviews_short$label[ids_train] %>% as.factor()  # train set labels
val_x <- reviews_dfm[-ids_train, ]  %>% as.data.frame() # test set data
val_y <- reviews_short$label[-ids_train] %>% as.factor() # test set labels
trctrl <- trainControl(method = "cv",
number = 5)
svm_mod <- train(x = train_x,
y = train_y,
method = method,
trControl = trctrl)
svm_pred <- predict(svm_mod, newdata = val_x)
svm_cmat <- confusionMatrix(svm_pred, val_y, positive = 'positive')
#print(svm_cmat)
return(svm_cmat$overall[["Accuracy"]])
}
acc_20 <- run_svm(0.2)
acc_20 <- run_svm(0.2)
acc_50 <- run_svm(0.5)
acc_70 <- run_svm(0.7)
cat('\n',
"SVM-Linear Accuracy 20%:",  acc_20, '\n',
"SVM-Linear Accuracy 50%:",  acc_50, '\n',
"SVM-Linear Accuracy 70%:",  acc_70, '\n'
)
acc_radial <- run_svm(0.7, 'svmRadial')
acc_radial <- run_svm(0.7, 'svmRadial')
cat("\n SVM-Linear Accuracy 50%:",  acc_radial, '\n')
library(text2vec)
knitr::opts_chunk$set(echo = TRUE)
# import libraries
rm(list = ls())
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Homework/Homework3")
libraries <- c("dplyr", "quanteda", "quanteda.corpora", "ldatuning", "topicmodels",
"ggplot2", "stm", "tm", "factoextra", "text2vec", "lsa", "bursts")
lapply(libraries, require, character.only = T)
# import libraries
rm(list = ls())
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Homework/Homework3")
libraries <- c("dplyr", "quanteda", "quanteda.corpora", "ldatuning", "topicmodels",
"ggplot2", "stm", "tm", "factoextra", "text2vec", "lsa", "bursts")
lapply(libraries, require, character.only = T)
news <- corpus_subset(data_corpus_immigrationnews,
paperName %in% c("telegraph", "guardian", "ft", "independent", "express"))
table(news$documents$paperName)
load('custom_stopwords.RData')
news_dfm <- dfm(news, remove = custom_stopwords, remove_punct = TRUE, stem = TRUE, tolower = TRUE, remove_numbers = TRUE)
news_dfm <- dfm_trim(news_dfm, min_termfreq = 30, min_docfreq = 20)
news_dfm
##### MODEL CODE #####
#news_tm <- LDA(news_dfm, k = 25, method = "Gibbs",  control = list(seed = 1234), iter = 3000)
#saveRDS(news_tm, 'news_tm')
#save.image('myworkspace.RData')
load('myworkspace.RData')
news_tm@loglikelihood
top_terms <- get_terms(news_tm, 10)
print(top_terms)
likely_topic <- topics(news_tm)
topics <- data.frame(table(likely_topic))
topics <- topics[order(topics$Freq, decreasing = TRUE),]
row.names(topics) <- NULL
print(topics)
top_5 <- c(topics$likely_topic[1:5])
for(topic in top_5){
top <- paste('Topic', topic)
print(top)
print(top_terms[,top])
}
which.max2 <- function(x){
which(x == sort(x,partial=(25-1))[25-1])
}
plot_top2 <-function(name){
ft <- which(news$documents$paperName == name)
first <- ft[1]
last <- tail(ft, n=1)
doc_topics <- news_tm@gamma[first:last,]
days <- as.numeric(news$documents$day[first:last])
doc_topics <- t(doc_topics)
max <- apply(doc_topics, 2, which.max)
max2 <- apply(doc_topics, 2, which.max2)
max2 <- sapply(max2, max)
top2 <- data.frame(top_topic = max, second_topic = max2, date = days)
top2 <- top2[order(top2$date),]
title = paste("Top 2 Topics for", name)
ft_plot <- ggplot(top2, aes(x=date, y=top_topic, pch="First"))
ft_plot + geom_point(aes(x=date, y=second_topic, pch="Second") ) +theme_bw() +
ylab("Topic Number") + ggtitle(title) + geom_point() + xlab(NULL) +
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
}
plot_top2('ft')
plot_top2('guardian')
names = c("telegraph", "guardian", "ft", "independent", "express")
data = c()
i=1
for(name in names){
vals <- which(news$documents$paperName == name)
first <- vals[1]
last <- tail(vals, n=1)
mean_vals = c()
for(topic in top_5){
doc_topics <- news_tm@gamma[first:last, topic]
mean_vals <- append(mean_vals, mean(doc_topics))
}
data[[i]] = mean_vals
i = i+1
}
tab <- as.data.frame(data, row.names = names, col.names = top_5)
print(tab)
#news_tm_2 <- LDA(news_dfm, k = 25, method = "Gibbs",  control = list(seed = 2000), iter = 3000)
#saveRDS(news_tm_2, 'news_tm_q2a')
#save.image('myworkspace_q2.RData')
load('myworkspace_q2.RData')
news_tm_2@loglikelihood
q2 <- function(tm1, tm2){
dist_1 <- tm1@beta
dist_2 <- tm2@beta
data2 = c()
for(row2 in 1:nrow(dist_2)){
sims <- c()
for(row1 in 1:nrow(dist_1)){
cosine_val <- cosine(dist_2[row2, ], dist_1[row1,])
sims <- append(sims, cosine_val)
}
data2[[row2]] = (which.max(sims))
}
cosine_sim <- data.frame(data2)
colnames(cosine_sim) <- 'Model1_Topic'
cosine_sim$'Model2_Topic' <- rownames(cosine_sim)
cosine_sim
top_terms1 <- get_terms(tm1, 10)
top_terms2 <- get_terms(tm2, 10)
commons<- c()
for(topic in 1:nrow(cosine_sim)){
list2 <- top_terms2[, topic]
list1 <- top_terms1[, cosine_sim[topic,1] ]
commons <- append(commons, (length(intersect(list1, list2))))
}
print(cosine_sim)
cosine_sim$common <- commons
return(cosine_sim)
}
sim_table <- q2(news_tm, news_tm_2)
print(sim_table)
cat('Average Common Words', mean(sim_table$common))
##### MODEL CODE #####
#news_tm_5_1234 <- LDA(news_dfm, k = 5, method = "Gibbs",  control = list(seed = 1234), iter = 3000)
#news_tm_5_2000 <- LDA(news_dfm, k = 5, method = "Gibbs",  control = list(seed = 2000), iter = 3000)
#saveRDS(news_tm_5_1234, 'news_tm_5_1234')
#saveRDS(news_tm_5_2000, 'news_tm_5_2000')
#save.image('myworkspace_q2d.RData')
load('myworkspace_q2d.RData')
news_tm_5_1234@loglikelihood
news_tm_5_2000@loglikelihood
sim_table2 <- q2(news_tm_5_1234, news_tm_5_2000)
cat('Average Common Words', mean(sim_table2$common))
##### MODEL CODE #####
#news_stm_content <- stm(news_dfm, K=0, prevalence = ~paperName + s(date), content = ~paperName, init.type = 'Spectral', seed = 2000)
#saveRDS(news_stm_content, 'news_stm_content')
#save.image('myworkspace_q3.RData')
load('myworkspace_q3.RData')
plot(news_stm_content, type = 'summary')
plot(news_stm_content, type="perspectives", topics = 49)
prep <- estimateEffect(c(49) ~paperName + s(date) , news_stm_content, metadata = news$documents)
plot(prep, "date", news_stm_content, topics = 49,
method = "continuous", xaxt = "n", xlab = "Date")
data <- corpus_subset(data_corpus_ukmanifestos, Party %in% c('Con', 'Lab'))
lab_con_dfm <- dfm(data,
stem = T,
remove = stopwords("english"),
remove_punct = T)
lab_index <- which(data$documents$Year == 1979 & data$documents$Party == 'Lab' )
con_index <- which(data$documents$Year == 1979 & data$documents$Party == 'Con' )
manifestos_fish <- textmodel_wordfish(lab_con_dfm, c(lab_index, con_index)) # second parameter corresponds to index texts
textplot_scale1d(manifestos_fish, groups = data$party)
summary(manifestos_fish)
words <- manifestos_fish$psi
weights <- manifestos_fish$beta
plot(weights, words)
x <- manifestos_fish$theta
y <- ifelse(data$documents$Party == 'Lab', 1, 0)
lin_model <- lm(y ~ x, data = lab_con_dfm)
summary(lin_model)
## From Lab 13
bursty <- function(word, DTM, date) {
word.vec <- DTM[, which(colnames(DTM) == word)]
if(length(word.vec) == 0) {
print(paste(word, " does not exist in this corpus."))
return()
}
else {
word.times <- c(0,which(as.vector(word.vec)>0))
kl <- kleinberg(word.times, gamma = 0.5)
kl$start <- date[kl$start+1]
kl$end <- date[kl$end]
max_level <- max(kl$level)
plot(c(kl$start[1], kl$end[1]), c(1,max_level),
type = "n", xlab = "Time", ylab = "Level", bty = "n",
xlim = c(min(date), max(date)), ylim = c(1, max_level),
yaxt = "n")
axis(2, at = 1:max_level)
for (i in 1:nrow(kl)) {
if (kl$start[i] != kl$end[i]) {
arrows(kl$start[i], kl$level[i], kl$end[i], kl$level[i], code = 3, angle = 90,
length = 0.05)
}
else {
points(kl$start[i], kl$level[i])
}
}
print(kl)
}
}
news <- readRDS('news_data.rds')
news_corpus <- corpus(news$headline)
news_dfm <- dfm(news_corpus)
bursty("obama", news_dfm, as.Date(news$date))
bursty("korea", news_dfm, as.Date(news$date))
bursty("afghanistan", news_dfm, as.Date(news$date))
news_data <- readRDS("news_data.rds")
table(news_data$category)
set.seed(2000)
news_samp <- news_data %>%
filter(category %in% c("WORLD NEWS")) %>%
group_by(category) %>%
sample_n(1000) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
news_dfm <- dfm(news_samp$text, tolower = TRUE, remove_punct = TRUE, remove = stopwords("english"))
news_mat <- convert(news_dfm, to = "matrix")
news_pca <- prcomp(news_mat, center = TRUE, scale = TRUE)
pc_loadings <- news_pca$rotation
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, 5, loading),top_n(pc1_loading, -5, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
pc1_loading
news_mat_lsa <- convert(news_dfm, to = "lsa") # convert to transposed matrix (so terms are rows and columns are documents = TDM)
news_mat_lsa <- lw_logtf(news_mat_lsa) * gw_idf(news_mat_lsa) # local - global weighting (akin to TFIDF)
news_lsa <- lsa(news_mat_lsa)
news_lsa_mat <- as.textmatrix(news_lsa)
america <- associate(news_lsa_mat, "america", "cosine", threshold = .7)
america[1:5]
corruption <- associate(news_lsa_mat, "corruption", "cosine", threshold = .6)
corruption[1:5]
pretrained <- readRDS("glove.rds") # GloVe pretrained (https://nlp.stanford.edu/projects/glove/)
# function to compute nearest neighbors
nearest_neighbors <- function(cue, embeds, N = 5, norm = "l2"){
cos_sim <- sim2(x = embeds, y = embeds[cue, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # cue is always the nearest neighbor hence dropped
}
america_glove <- nearest_neighbors("state", pretrained, N = 5, norm = "l2")
america_glove
corruption_glove <- nearest_neighbors("welfare", pretrained, N = 5, norm = "l2")
corruption_glove
library(rjson)
json_data <- fromJSON(file='Webscraping Code/raw_content.json')
json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
getwd()
setwd("~/Documents/NYU/Spring2020/DSGA 1015/Project/TaD_Final_Project")
getwd()
json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
urls <- json_data$urls
urls <- json_data$url
rm(list = ls())
library(rvest)
library(dplyr)
scrapetext <- function(url, filename){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
speakers <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[following-sibling::br]") %>%
rvest::html_text() %>%
unique()
speaker_fname = paste(filename, 'speakers.txt', sep = '_')
text_fname = paste(filename, 'text.txt', sep = '_')
write_file(speaker_fname, speakers)
write_file(text_fname, text)
}
write_file <- function(fname, data){
fileConn<-file(fname)
writeLines(data, fileConn)
print('Saved')
close(fileConn)
}
library(rjson)
json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
View(json_data)
df <- lapply(json_data, function(url) # Loop through each "play"
{
data.frame(matrix(unlist(url), byrow=T))
})
View(df)
# Now you have a list of data frames, connect them together in
# one single dataframe
df <- do.call(rbind, df)
View(df)
View(json_data)
df <- lapply(json_data, function(url) # Loop through each "play"
{
data.frame(matrix(unlist(url), ncol = 2, byrow=T))
})
View(df)
# Now you have a list of data frames, connect them together in
# one single dataframe
df <- do.call(rbind, df)
View(df)
# Make column names nicer, remove row names
colnames(df) <- (c('url', 'title')
# Make column names nicer, remove row names
colnames(df) <- c('url', 'title')
# Make column names nicer, remove row names
colnames(df) <- c('url', 'title')
rownames(df) <- NULL
View(df)
df$title <- gsub(' ', '-', df$title)
View(df)
df$title <- gsub(' ', '-', df$title) %>% tolower()
View(df)
df$url <- Map(paste, url_prefix, df$title)
url_prefix <-' https://www.rev.com/blog/transcripts/'
df$url <- Map(paste, url_prefix, df$title)
View(df)
df$url <- Map(paste, url_prefix, df$title, sep = "")
View(df)
json_data <- fromJSON(file='WebscrapingCode/raw_content.json')
df <- lapply(json_data, function(url) # Loop through each "play"
{
data.frame(matrix(unlist(url), ncol = 2, byrow=T))
})
# Now you have a list of data frames, connect them together in
# one single dataframe
df <- do.call(rbind, df)
# Make column names nicer, remove row names
colnames(df) <- c('url', 'title')
rownames(df) <- NULL
url_prefix <-' https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
df$url <- Map(paste, url_prefix, titles, sep = "")
View(df)
df$title[grep("t", df$name) ]
df$title[grep("Donald", df$name) ]
df$title[grep("Donald", df$title) ]
df$title[grep("Trump", df$title) ]
df$title[grep("Governor", df$title) ]
states <- df$title[grep("Governor", df$title) & !grep("Trump", df$title) ]]
states <- df$title[grep("Governor", df$title) & !grep("Trump", df$title) ]]
states <- df$title[grep("Governor", df$title) & !grep("Trump", df$title) ]
states
states <- df$title[grepl("Governor", df$title) & !grepl("Trump", df$title) ]
df$title[grepl("Governor", df$title) & !grepl("Trump", df$title) ]
df_countries <- df[!grepl("Governor", df$title) | grepl("Trump", df$title) ]
df_countries <- df[!grepl("Governor", df$title) | grepl("Trump", df$title), ]
View(df_countries)
df_countries <- df[!grepl("Governor"|'Gov', df$title) | grepl("Trump", df$title), ]
df_countries <- df[!grepl("Governor", df$title) | !grepl("Governor", df$title) | grepl("Trump", df$title), ]
View(df_countries)
df_countries <- df[!grepl("Governor", df$title) | !grepl("Gov", df$title) | grepl("Trump", df$title), ]
View(df_countries)
df_countries <- df[!grepl("Governor", df$title) | !grepl("Gov.", df$title) | grepl("Trump", df$title), ]
df_countries <- df[(!grepl("Governor", df$title) | !grepl("Gov.", df$title) | grepl("Trump", df$title)), ]
df_countries <- df[(!grepl("Governor", df$title) & !grepl("Gov.", df$title) | grepl("Trump", df$title)), ]
View(df_countries)
df_countries <- df[(!grepl("NYC", df$title) & !grepl("New York", df$title) | grepl("Trump", df$title)), ]
df_countries <- df[(!grepl("Governor", df$title) & !grepl("Gov.", df$title) | grepl("Trump", df$title)), ]
df_countries <- df_countries[(!grepl("NYC", df_countries$title) & !grepl("New York", df_countries$title) | grepl("Trump", df_countries$title)), ]
df_countries <- df[(!grepl("Governor", df$title) & !grepl("Gov.", df$title) | grepl("Trump", df$title)), ]
df_countries <- df_countries[(!grepl("NYC", df_countries$title) & !grepl("New York", df_countries$title) | grepl("Trump", df_countries$title)), ]
df_countries <- df_countries[(!grepl("Gavin", df_countries$title) & !grepl("Ohio", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
df_countries <- df_countries[(!grepl("Arkansas", df_countries$title) & !grepl("Virginia", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
df_countries <- df_countries[(!grepl("Organization", df_countries$title) & !grepl("WHO", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
df_countries <- df_countries[(!grepl("IMF", df_countries$title) & !grepl("Louisiana", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
df_countries <- df_countries[(!grepl("Bill", df_countries$title) & !grepl("Kansas", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
df_countries <- df_countries[(!grepl("Minnesota", df_countries$title) & !grepl("Mayor", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
df_countries <- df_countries[(!grepl("Utah", df_countries$title) & !grepl("Jersey", df_countries$title) | grepl("Trump", df_countries$title)), ]
df_countries <- df_countries[(!grepl("Biden", df_countries$title) & !grepl("Mayor", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
df_countries <- df_countries[(!grepl("Cuomo", df_countries$title) & !grepl("Santis", df_countries$title) | grepl("Trump", df_countries$title)), ]
View(df_countries)
View(df_countries)
toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task')
matches <- unique (grep(paste(toMatch,collapse="|"),
myfile$Letter, value=TRUE))
matches <- unique (grep(paste(toMatch,collapse="|"),
df$title, value=TRUE))
matches <- df[(grep(paste(toMatch,collapse="|"),
df$title, value=TRUE)), ]
View(matches)
paste(toMatch,collapse="|")
paste(toMatch,collapse='|')
matches <- df[(grepl(paste(toMatch,collapse='|'),
df$title, value=TRUE)), ]
matches <- df[(grepl(paste(toMatch,collapse='|'),
df$title)), ]
View(matches)
toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.')
matches <- df[(grepl(paste(toMatch,collapse='|'),
df$title)), ]
View(df_countries)
row.names(matches) <- NULL
View(matches)
scrapetextonly <- function(url){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
return(text)
}
texts <- list()
for(url in matches$url[1:3]){
trans <- scrapetextonly(url)
texts <- append(texts, trans)
}
matche$url[1]
matches$url[1]
url_prefix <-'https://www.rev.com/blog/transcripts/'
titles <- gsub(' ', '-', df$title) %>% tolower()
df$url <- Map(paste, url_prefix, titles, sep = "")
toMatch <- c('Trump', 'Trudeau', 'Morrison', 'Kingdom', 'Britain', 'Zealand', 'Boris', 'Task', 'U.S.')
matches <- df[(grepl(paste(toMatch,collapse='|'),
df$title)), ]
row.names(matches) <- NULL
scrapetextonly <- function(url){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
return(text)
}
texts <- list()
for(url in matches$url[1:3]){
trans <- scrapetextonly(url)
texts <- append(texts, trans)
}
View(texts)
for(url in matches$url[1]){
trans <- scrapetextonly(url)
print(trans)
texts <- append(texts, trans)
}
toString(trans)
scrapetextonly <- function(url){
transcript <- read_html(url)
text <- transcript %>%
rvest::html_nodes(xpath = "//div[contains(@class, 'fl-callout-text')]") %>%
xml2::xml_find_all("div/p/text()[preceding-sibling::br]") %>%
rvest::html_text()
return(toString(text))
}
texts <- list()
for(url in matches$url[1:3]){
trans <- scrapetextonly(url)
texts <- append(texts, trans)
}
View(texts)
View(texts)
length(texts[1])
